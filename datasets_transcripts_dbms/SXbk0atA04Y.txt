sleep Sir class is not so boring I make it interesting okay so today what we are going to do is we are going to study something called as working with structure data we are going to today learn how to use data frame and data set okay two important concept that comes as a part of spark is called as data frame and data set if you know a basic SE statement then it is very easy to understand even if you don't know it we will go through the exercise I will walk you through handson how to do it and then you can try it out using the assignment and practice it Okay basically working with structure data as I told you there are two forms of basic data set one is structured and another is unstructured and something in between is called a saving structure right structured data set is a data set which has got specified format like data is separated by comma like CS file data is separated by pipe sign you know pip right data is separated by tab data is separated by space so when you import the data to the database table it automatically gets recited into respective columns right that is a structure data same structure data I told you some portion of the data will be structured some will be unstructured for example email you will have two and from from you have to follow certain structure format but the content can be unstructured people can write any form of content right and then you have uh completely unstructured data means random lock files right so today we will learn about structure data how it can be used in spark okay what we are going to do is we are going to create something called as a data frame I will explain you what it is Data frame and how we leverage that data frame usage to do some computations do some calculations okay again this is very important in terms of if you are thinking of learning machine learning data frame is a very important concept okay you will be able to understand how to manipulate the data and I will explain you why data frame is basically needed right what is the benefit of having data frame how it helps to visualize the data how it helps you to manipulate the data once it is uh represented in that format clear as usual ask me a question if you have this is the source of information where the details have been taken good to read just spend like 15 20 minutes to read about it but anyways I'm trying to go my today's class will be more on showing you how to do it okay I will go quickly to the slide text because many of the contents are being revised as part of the previous conversation okay SQL spark SQL is a spark module for structur data processing again it's similar to the SQL right unlike the basic spark rdd API the interface provided by spark SQL provides spark with more information about structure of both the data and the computation being performed basically spark SQL is one of the component of spark okay think about it's like an SQL editor where you can type your query extract the data and pull the data into certain format okay why we needed think about why we need spark SQL or why we need any specific hql and those type of statement because when you do a storage in the data in a structured way in the database you store the data into tables and columns right and the table and column reside in one database right in case of big data is data residing in one database no it's split across multiple places in order to collect the information from multiple sources of data nodes you need a way where you can pull all the information and represent it into form of a table and column then only you will understand the table right so think about this way if I have a database where the table has got student information right in a normal database right let's say there is a table called a student so I will have first name last name role number address phone number all those columns right and I will insert the data in that particular table right now if I tell you that my student data is not in one table but it is split my first name and last name is sitting on one data node right my home address phone number and location is sitting on the other data node and my let's say subject and course whatever he studies is sitting on another data node now you could pull all the three information and show the student information in a single table right I need some way right because your data is split in multiple places it's not sitting on one table so in that case your Spar SQL comes into two important concept you have to understand one is called as data set another is called as data Frame data set is a distributed collection of data means if your data is sitting in node one node two node three node four if you have to pull the data from all the data nodes then the data which is arriving to you is called as data set it's a collection of data coming from different distributed sources data set is a new interface added in spark 1.6 that Pro provides the benefit of rdd that is strong typing ability to use powerful Lambda function with the benefit of spark SQL optimized execution engine if you remember last time when we discuss about rdd right how many of you were there in the class when I explained you that finding the prime number creating rdd1 rdd2 rdd right yes basically the rdds are what there are multiple different streams of process that can run in parall each rdd will be executed separately think about if that rdd is collecting the data from different sources right my rdd will collect the information from different sources when it collected it combine all together that information of data is called as data set clear now once you have got the data set from different sources you have to have a representation right computer has done it for you it has collected it for you but how would you see it so you need a logical way to see it that view logical way to see is called as data it is a representation of that collection of data in a form of a table and column right so let me explain you all so that you understand before I to the next how many of you understood what I'm saying H how many of you didn't understand at all so think about this way if your one data is stored here right this is node one where data source is one there is another data source that is data source two and there is another source of data it's called a data source 3 okay this has got some information this has got some information this has got some information right in the previous class we discussed about rdd res distributed database we created a resilient distributed database with an example of prime number right numbers which are divisible right you have one file so let's say there are three files and I want to check in every file which numbers are divisible I created a parallel rdds right now rather than prime number this time let's say your data has got lot of information about student so student let's say there is 100 of students right few of information is stored here same student few information is stored here few information of student is stored here same student set right now I want to see the data what I will do I will first pull the data from all the different sources when I'm pulling the data from all the different sources the collection of data which I get is called as data set collecting all the information now since this information is collected this is collected in some format some format either by comma separ or with special character whatever it is is user I want to see it if I represent that data into form of first name last name home address phone number and I start putting the data in this format [Music] called data set once the data set is created the next step is to create data Frame data frame is part okay how many of you people understand the concept in database uh table and views temporar right sorry views were temporary what when the temporary like you created a view or a special like to to use the a table or a certain amount of values from the table and then you can start you wouldn't you couldn't like you wouldn't stay safe something like that you right but partially okay to understand very correctly so table is what table is all the information about the details so I have a let's say I have a student table with all the student information right and other table which has got let's say there is a student table which has got students College information and there is the same student which has got student personal information okay if I have an access to students uh College information what I will do select star from student table and get all the information of student right but if I'm not willing to expose my personal information of a student personal data what should I do I should have a way where I can show what they want to know not the entire table right so basically view is a logical structure which is created to allow users when I say user means the end user to see what they are entitled to see for example let's say I I give you the views are very commonly used in database again little bit out of the topic so let's say there is one table called as student here right okay there is another column which has got let's say XYZ table right and there is some other table called as pqr okay there are three tables right each table will have columns and rows let me know you all are able to understand or not add three tables right clear each table has got rows and columns okay let's say this is one application when this is I'm saying this is front end application okay UI now this front end application is having access to this table right let's say this is you as a user you have access to this table of student I you can see all the information of the student right and let's say there is another application which is created which has access to this table okay in order for this person to see this table information in order to have this person see this table information of a particular column I only want him to see this column on this table you're getting it I don't want him this person to see everything what is inside it I want him to see only a specific column which is reciting here this portion I only want him to see right so what should I do if I give him the table access he can see every column right yeah I don't want him to see every column I want to restrict him to see only what I want him to show so that he can use and continue his application whatever he wants to use it right so what I will do is I will create a view I will say expose this column right and expose this column whatever he has got here to view it and create a logical table here so that he can directly access this view understood those are views views are generally created first of all it is a logical table it's not a temporary table it's a logical table which is executed on demand so if I create a view it doesn't means it will create a table for you it will only create a logical table for you when asked so if your friend frontend application code is written to execute an specific stored procedure normally it is done through stored procedure you create a stor procedure stor procedure will create a logical View and it will pull the data what is to be made visible to you and then you play around with that data clear mostly views are read only it can be right but mostly views are created for logical read clear any questions can there be a situation where the view comes from that same table the user has access to say pqr yes then in that pqr you can still create a view inside that pqr for the user see something but not view so I will tell you the example most of the time whenever you are accessing the database in Enterprise right if you're not updating or inserting the data you are always accessing for read for views we don't allowed user to directly read the data from the table because when you're reading the data from the table you are connecting to the database and using the connection to read the data if you're creating a views multiple people can access a view at the same time right without overloading your database so like normal banking system or anything any time we we we maybe log on to Mobile Banking or log on to online mean you're using not actually getting the data as long as you're not inserting or updating or deleting the data that has to happen in the table level it cannot happen at the views I'll give you example right now we are working on a project where we have it's it's a application which RBC internally is building right and we are going to send that data to a company called as pure fact okay it's a third party company right now one option is that we tell them okay come and connect to our database and pull whatever you want should I do that no it's a Bach of privacy right other option is I will tell okay what you want I will create a separate table for them creating a subset of that table creating that table pushing the data and tell them now you read it from here but there is an overhead if I create a separate table for them then I have to continuously update it with the new value right isn't it if the data changes in the Code table then I have to go there and update there also plus I have to let them know that yes I'm updating too much work too much of work rather than what I did I created 200 views for them okay what you need okay you need this column information this column information I go back and check in my database okay these columns are appearing in 1 2 5 15 20th table pull that column name created a logical view right and now whenever they are hitting our database to to our system to get the data we are refreshing our data from our source table whenever they hit that views store procedure is called and we are giving the data from The View understood views help to optimize the performance of the database also okay clear everyone data frame is organizing a data set into a form of a tables and problem in a particular format is called as data frame clear databases in Big Data databases are source of structure data in many cases uh that will be relational database but can also be other database management system it can be considered as a part of big data link included into big data and data analytical tool anything to be explained here it's just a theory I don't like reading that okay there are many things which are theoretical I will read it just pause me if you don't understand particular line okay let me read slowly okay tools to operate with relational database hadu input output data format provides the option to retrieve the data from resilient distributed database and send the data to the resilient distributed relational database management system how do input output uh data format is a method DB input format DB output format DB input format will read the data input the data from a database and DV output format will be once the data is read is spit out the data which is to be consumed by other system okay hi is a separate component that belongs to Hado EOS system and can be used to operate with data via logical data structure and language similar to relational database and SQL hi is also SQL right but it operates in dis right when you installed hi over Hado what did you do you executed overo right it creates a logical data model apply to the data stored in hdfs high SQL inspired language separates the user from the complexity of map reduce programming I I gave you the explanation of it right you write a high query internally if you remember the architecture right what it used to do it used to actually internally execute map reduce program but for the front end of high query you just sending a query a patches spark spark SQL tool to work with the relational database ability to incorporate many other types of data databases under spark Frameworks SP framework also is used to not only run over regular database it can run on Hardo which can run on other type of databases also programming languages used are pql with pyodbc other languages like Java C and other similar Solutions are also available for operating the uh relational datab base spark SQL AP spark SQL integrates relational processing with the spark function programming it is a spark module for structure data processing spark SQL takes its place between rdd and relational table this is addition to the basic spark rdd API unlike the basic spark rdd API the interface provides by the spark SQL provide spark with more information about the structure and both the data and the computation being performed so spark API basically when you write the rdds you are processing that to the databases or to the Hado cluster Downstream right the spark that those those processing capabilities are done through spark API use a spark SQL to execute spark queries Spark SQL can also be used to read the data from the existing installation so if you have Hado hdfs file system above it you have hql and above that you have install spark so you can use spark to call Hive Hive to call Hado hdfs file system underlying it okay only difference between hi SQL and the spark is hi run on the desk the query runs on the desk spark runs on the in memory okay feature of spark SQL supports distributed in memory computation on a huge scale it can easily execute SQL query through it in addition to read the data from existing H installation we can use SPX SQL the result comes as a form of a data set and data frame today we will see how the data set and data frames are appearing in the UI okay when SQL is run on another programming language we can interact with SQL interface by using the command line over jdbc and odbc last class we discussed about Thrift right Thrift and jdbc and odbc connectors right right which internally used to call H server right and within H server you used to have uh compiler you used to have Optimizer you have used to have what was that uh metadata right spark SQL feature integrates UniFi data access and high compatibility use of spark one use of spark SQL is to execute SQL queries spark SQL can be used to read data from existing H tables when running SQL within the this this is I already explained you right the previous slide is same repeat spark data SQL data frame versus SQL data set just remember data frame versus data set is data frame is representation in form of a table and column in UI versus data set is just a collection of data before it being represented in form of a table and column clear everyone is clear till this point yeah see the only important topic this slide I explain use two data frame and data data set right data set is a collection when you collect the data from multiple Source but now the data is collected you need to view it you need to see it when you want to see it in the form of a particular format that is called as data frame doesn't mean that data set is not structured dat structure together data set is structured for for the for the for the software not for you okay machine understands the data set collection but how will you understand it you need so think about if I give you a huge CSV file with comma separated huge of 100 columns and give it to you and say can you explain me what it contains you'll need to spend time to understand otherwise you will not understand what it contains right but if I take that same data set and put it into a column and then I will name each column with a column header and then put the data in each column and then show it in a table format will you understand it same thing data set and data frame there is a use of converting into Data frame I just don't want to go to that topic because it's a part of machine learning okay let me Hint it right now so what why we are converting into Data frame to view it into table and column okay example I will give you think about I am measuring the temperature of Canada Toronto area for last 50 years right I I'm I'm noting down the temperature of Canada Toronto's City for last 50 years for every AR understood someone would be recording it right you see that site website right temperature today 1 Hour 2 hour right somebody is recording it so think about if a person is trying to record the data from last 50 years in a day how many hours are there 24 hours so there will be 24 columns in the row there will be dates date first date second date third date right and this degree Centigrade will be there for every hour let's say now this information is there in table in column it may have happened that someday the person is absent has not recorded it it can happen right it can also happen that suddenly that there was a tornado the person was not there for full day the entire office was closed you don't have the data so what should I write there what value should I write on those column 1 R 2 r 1 a.m. 2 a.m. 3 a. 4m. nothing nothing isans what Z zero you cannot write in C you cannot write zero right so what should you write nothing right now if I if I have that situation in last 50 years 4 40 times let's say I have accumulated a data for 55,000 days out of that 5,000 10 times were there I had no data that field was empty right correct yeah if I take if I ask you I want to see what is the average temperature at 7 a.m. in Last 5 Years what should I do I to select that seven column right from top to bottom and do average in Excel how do you do you do equal to average and you select top to bottom and close the parenthesis is that data right no why empty spes there empty there may be 10 instances where the value of that particular data is not there right now I'm talking about temperature right when you take 5,000 data points and four five is not that your average will not be affected that much right it will very close hardly 0 one% will be the deviation right I don't need that deviation let's say you you have a data set where you cannot make an error your error probability is let's say 99.9999% should be correct then this doesn't work right if I'm collecting a data of of let's say what you say a very sensitive data which is which cannot be uh which cannot accept any error or can accept very very low error rate and if five or six data of that sort is empty what will you do you take those five hours and you focus on the why will you take out because there's no figure there no you cannot take it out how will you let machine that's the reason we are going to learn machine learning how to handle that situation and for that you create that data frame when you create a data frame you a you filter it out to see which columns are empty and then you do some processing which is called as pre-processing to make sure that data is if it is empty how to fill that data when I'm filling that data I should be make sure that I filling the data which is very very likely to to be happening that day so that when I take average it doesn't skew my number but I have to see it right how will I see that data I need some format of frame right because I gave you an example I'm pulling the data from 10 different sources 10 different sources let's say my column is like 300 columns how would you read it you canot scroll one by one and see where zero is coming no you cannot do that you need a mechanism where you can see it in table form and then you can use some function to determine where zeros are coming and then you can use that zero to be substituted by some values which machine can use to train itself we'll learn about machine learning that's the whole concept behind what is data frame okay don't worry we will come to that topic okay spark data frame can be created from many Source you can create a data frame from rdd that is resilient distributed data set last class when we did that example for primary number right I gave you rdd1 rdd2 I pass that array list right I told you rather than passing array list Pass the full path where the file is stored right and then you can run rdd1 rdd2 RD3 for prime number on those files right so you can use rdd to get the data frame created you can use list to create a a uh data frame CSV file Hive table text file hbase table ration database table you can use any of them to show how the data frame can data set right because data set not internally it will create data set and then you will see it in the data frame now right you can see more details about here spark creates data frame with example you may convert existing rdd to data frame manually via functional DF we'll see how to do it okay data frame may be created from data object such as list a data frame is equivalent to the relational data table in the SQL database data frame stores a data into tables data frame is similar identical to a table in a relational database but has got richer optimization richer optimization means any idea anyone can highlight on this I'm saying the data frame is same as data table in SQL you have tables right right rows and columns right I'm saying data frame is same as data table in the database but it supports richer optimization what does it mean so talk about manage resources so richer optimization means let's say I have created a table in a database right I have entered the data inside it all the column have been filled with data now okay now tomorrow I say to you that look I want to add two more columns in the table what are you going to do INS INS I'm saying adding two columns so let's say again you you I know it's better topic but you need all to understand if you're studying the subject need to understand this let's say there is a table okay I'm drawing a table it has got primary key as as a role number okay first name last name the table name is student okay I fill I give the number role number is 1 X Y role number two is pqr x y z okay rule number three has AB b c and bf I can have the data like this right this is a table name column name column name one column name two column right and this is the data inserted it right clear I'm working with the table now suddenly the person comes to me and I say that oh can can I because this table is incomplete I want to add two more column one column is address other column is let's say phone number and this should not be null understood not null you understand right yes you cannot be empty right what should I do you have to create fs and how many of you say that there is a command in the database called as alter alter table table alter table table name is student Al table table name uh let's say address V care and phone number number 10 can I do this yeah number 100 not n can I do this can I alter the table in this scenario come on guys tell me yes or no that's it how many of you say it cannot be done okay how many of you say can be done okay why should it it how why do you say it can be done any guesses have a table and the the language we using is SQL think logical if I do alter table table name address where four number is not null table is trying to alter it means trying to change it but this column say not null data is already inserted inside it are you passing any data to it then how will it allow you to do it are you all getting it this is the two column you are saying I want to add to the table but you are saying this cannot be zero or not null right I'm trying to alter the table with new columns but I'm also saying this cannot be null but if this is not null if the table get altered what value you should put for existing data database will not do put work for you to put something n there will it automatically put it no you have to put it and when you say that oh I don't have the value for it it says it cannot be null it will throw an error it will say that alter table cannot happen because the table is defined as not null column is defined as not null right how to do it then you do and you you know you take out the notes now and then no but I that's requirement yeah you just need to put the default number like I don't know like not n default zero like that not not default zero no but like the code they like allow you to do that right to use the the like the SQL code if you put the fold there is a number I know I know that their information is not right no but you cannot put garbage data whatever you want right you cannot play around with database putting whatever you want yeah the information is not right but you can do that you cannot do this is logical error you cannot put anything right so if you cannot do this what option I should have you what you were saying what should I do yeah fill up the first how will you fill it up when you don't have the column itself on faster tell me a dirty way of doing it one is this option I put anything Val colum then put the value then no but you have already your water has already flown down the bridge now you have already entered three rows and now you're trying to update the two rows think tell me I'm surprised you people never face that problem what should I do one option is oh I did wrong I delete everything give me those answer now oh I did wrong I don't know that I have to add two tables let me delete all add column alter table now and now start retyping the statement insert insert insert insert right so if I have 5 million record then I have to do 5 million times right that's the worst dirty way right second option is create another table with that column create another table table with that so this is student I will create another table student one right which has got all the four columns with a new added columns with the new added columns and then I will start migrating the data to inside this right when I do migration I will do a check here that if phone number is empty put this valid phone number I can do this again very dirty way once that all the data is filled in the new table I delete the old one and rename the first one right yeah three-step process but these are not good way of doing it right not you think about it this is a practical problem you all face right theum of is too many yeah it is 5 million record then do that then whole day you will sit and do this there are mechanism to do here also okay but the good part here is when you dealing with spark right it supports richer optimization you can add the columns you can combine the columns I want to merge column b c d to form one column you can manipulate the colums you can update the column you can do the calculation of the column why because you are not doing in the actual table where are you doing it spark runs it in memory and the data is loaded already in the memory you can play around with the data you can add a column you can join the columns you can merge the column you can change the value of the column because everything is doing done there so in this scenario you can still add a column where the condition is not yes and we will be doing in machine learning correct so when we add the column we can merge the column we can substitute the column we can rename the column why because are you playing with the real data which is kept on data node you have already pulled out the data from those data nodes and brought it into the main memory as form for data set and then you representing into the data frame now that data frame is What It's Like A View to you right It's A View to you now you can twist the column you can rename the column you can update the column whatever you want to do you can do it why it is needed it is needed for machine learning algorithms right what the data frame is needed for machine learning alori mostly it is not mostly it is practically needed for machine learning and not for machine to learn it is for you to understand what you're doing on data yeah because I knew that the data set is where Ma the machine doesn't need machine will automatically work on your data set right but as a user you want to know what is happening in each columns until this you can understand the column what is it hold what is the value it is collecting it how to change and manipulate the data how will you do it but you want all this process to be very fast if that 20 million data loaded on the desk then it will take 10 minutes for your your data to be fetched if it is sitting on your main memory it will be quick right every time you execute the command it will be very quick to get you the result and you can start playing around with the program okay again this is not written in the book so try to pay attention what I'm explaining it is a data abstract in domain specific language applicable structure sem structure data it is a distributed collection of data in form of a name column and rows clear a data frame can be constructed using a function of spark schema will be generated and temporary data table will be constructed we can Leverage The reg registered temp table function to build temporary table to run on SQL command on our data frame using scalar language this is what I'm talking about so you can use SQL command to run on your data frame right right and this is all done in your main memory and you can also store it in a temporary table to process it is it language you can use other also R also it is conceptually equivalent to the table in relational database or the data frame in Python and R limitation in the lifetime only the limitation is once your table has been refreshed or your machine has restarted or your machine where main memories there getting restarted data is gone right because it's in the in memory right so that's the only limitation the lifetime of temporary table is tied to the session it creates an in memory table that is scoped to the cluster in which it is created so if you connect to the database if you're connecting to the uh database in hadu bringing that data to your main memory when I'm saying main memory we are talking about main memory of the cluster node not main memory of your machine okay so whenever you run your like in your lab it's on your own machine but when you do this thing in the cloud environment right so your data is stored in different nodes in the cloud right and you're bringing that data to your uh virtual machine which is given to you right virtual machine means uh cloud machines to execute and perform the action right now when you are working on that machine you are directly connected as a session to it once you log out the data which was stored in that virtual machine is is gone because the main memory has been deleted because the session has been terminated so all the temporary table and everything which you see gets removed okay SP SQL support operating on variety of data sources through data frame interface a data frame can be operated by using relational transformation and can be used to create temporary views registering a data frame as a temporary view allows to you to run the SQL queries over its data now we save yes you can very good question we are going to go that that slide the very next slide once you if you want to save the changes what you have created as part of data frame you can write it back to the data hdfs file system let's say you want you played around with that column and other things right and it is in main memory but you want to save it to be used in future right so what you do you can save that data frame back to hdfs file system so that when next time you want to retrieve you can retrieve it and use it again okay yeah we can run SQL queries in our table based in our data frame supports simple queries as well as aggregation filtering sorting subqueries and P it supports same type of function which you have in SQL aggregation function anyone know what are these everything aggregate function in database example no just a second unation Union I don't remember but most likely the the common ones are sum aggregate function is sum of the all the column I want to find the sum of all the students score for the course ABC right Agate function I want to find minimum of all the numbers on the column I want to find maximum M Max count aggregate sum standard deviation these are all aggregation function filtering filtering is intersection Union intersection Union intersection exclusion function right sorting is sorting datab s sending sort by sending sort by descending ASC and D DC right yes or no sir subqueries sub queries nested queries anyone remember queries you write this like this select from table name where and in the bracket you write again select Max number from the table right so this query will return one value which will be used here and will be consumed here right nested query subqueries joins left join outer join right joins yes words are good to remember right SP also support using jbcc command line argument which we have seen in the previous integrating with hi data can be stored using hi in memory column format there is also also operating save as table save as table this is your question right you asked right this is there is an also an operation save as table which creates a permanent physical table this table is accessible to all cluster so if you do play around with in memory table and now you want to save it back you can use this function save as table we'll take that table and write it back to the hdf permanent dis so that you can but that will be a new table right sorry that new that save as table is a new table is not that like you will update your previous table no it will create a new table it will not why it will never because this is your created table right that's why save as table if you don't provide a table name it will store with its own default table name so normally developers they will mistakenly write it save table and forgot to give the table name because it doesn't ask and Wars you for a table name right so it is like saving creating a new folder new folder one new folder two new it creates like that right so if you have to specifically you have to give the table name okay the table metadata including location file is stored in high meta store difference between these spark API based on various for example data representation immutability and interoperability that is also question where to use rdd data frame API data set of spot you read this website this link okay I'm not going to explain this this is more of a Theory read basically difference between spark API is based on various features like data representation form of data frame and data set immutable means immutable means undestroyable it's undestroyable if you create an rdd the structure never destroys right data gets destroyed interoperability means it can support programs created in one language and executed on other language also so if you have written a data machine learning program in Python you can use to execute it in other languages also using a defined compiler right spark rdd API I think I've already explained you spark rdd API last time this a simple spark definition right rdd stands for resilient distributed data set it is a partition read only partition collection records rdd is a functional data structure in spark it allows programmer to perform in memory computation or large cluster of data in fall to manner the speeds of the task only thing to remember is rdd API the first two points remember that it is resilient distributed data set and second it is a collection record which is executed in memory spark data frame API unlike rdd data organized into name columns data frame everyone is clear in of row and table you use spark data frame API we will see in the example okay spark data set API is the collection of records from different Source system okay these apis are underlying the spark you don't see it okay you don't see it you use it it's like import statement you're importing the spark apis to execute it oh God spark release version this this this data representation rdd a set of java scalar object representing data distributed on many machines and cluster data frame is distributed collection of data organiz again it name columns data set again it's the same thing repeated again and again data set format data source API again spark can consume the data read from any Source system okay so you can read the data from from spark coming from a text file on your machine coming from network drive from other machine coming from cloud coming from S3 coming from any place you you can download the data which is on internet also okay we'll do that example like I will show you how to bring the data set right as an example we'll bring the data set from a particular location and bring it to our local machine not local machine to our spark environment right and then we'll play around with that data set okay immutable and interoperability rdd each partition is immutable but can be transformed a given rdd into Data frame and other rdds data frames after creating object cannot be transformed data set data set allows to convert your existing rdd and data frames into data set optimization rdd cannot take a benefit from Catalyst which spark optimization component data frame optimization takes place using Catalyst Optimizer how many of you know Catalyst what does Catalyst mean when you say speed of that's what speeds up the process right anything anyone can add you're right what is catalyst causes a reaction it causes a reaction how many of you studied chemistry yeah reaction catalyst is a a component which uh makes the process more execution faster right correct yeah so if you take a particular chemical like how many of you have cars right your car has got something called as catalytic converter what it does it's not cars have that every car will have it every car last 20 years every car should have it must have it as part for the comp what is catalytic converter quickly this is out of topic but I'm just asking anyone knows so when you when you run your car what happens your exhaust the smoke comes out right yeah what should that smoke be done thrown out in the air yeah so there will be a pollution right yeah so what should I do so the smoke will contain lot of harmful particles right some are very harmful some are less harmful and some are very very less harmful right so what is my objective I should stop the harmful ones right right yeah in order to stop the harmful component particles these are mainly carbon monooxide sulfur phosphorus and all those poisonous one right I need to stop it I know sulfur phosphorus and all those things reacts with which chemical which which element lead right so if you put lead or if you put charcoal right charcoal in the smokes charcoal will absorb it right it is porous right but it will absorb with its own capacity right but I want to intensify the absorption of that so that it can absorb more and let very less pollutant to go outside so what I use I use along with lead in the cataly converter I use rare Metals inside it like those are very expensive that's why they say catalytic converter are stolen than in Canada right because it is very expensive so along with L there are rare elements inside that catalytic converter so what happens whenever you car smokes right the catalytic converter enhances the absorption power of lead so same thing spark does it it act as a catalyst Optimizer basically whatever execution is happening in Hive and in hdfs or in main memory it enhances that execution using cost optimization and dag process right it improves the process improves the execution time rdd is slower to perform simple grouping and aggregation operation data is easy and faster for large data set data set is faster many data set is just just aggregating I I'm not reading this this is more about hi you want me to read it how I'm just reading it in the important points okay hve is an open source software everyone knows it because you all can download and use it Hive provides SQL like declarative statement called as hql it is like same SQL but it is running in in disk not in memory okay it supports same function like your regular SQL right remember High SQL SQL and SP SQL are very much same okay only difference is when you're running in high iql you are running in desk when you're running the same SQL using spark you're running the same ql in in memory and when you are running the same SQL in a normal traditional database you're running in rdps clear so you use the same function conate means not add right subring means take split subit substring right if XYZ tqr is there if I take substring three so XY Z will be one and then split right round yes floor and ceing function right how many you know floor function going 38.5 38.6 should be converted to 39 means you doing a sealing function going up to the approximation floor function is 38 4 is to be converted to 34 that is ceiling floor function okay aggregation function like sum count Max Min average standard deviation and all those function Group by everyone knows in SQL Group by Clause sort bying so everything you do in SQL standard SQL statement you can run it in Spar SQL and high okay I have already explained last time thft application odbc application jdbc application in case of hi architecture right meta store used to storing information schema metast store typically resides in relational database we can interact with hype using method like web GUI Java database connectivity interface metadata is the data information of the table right and the columns and the relationship right it tells you where the data is stored which column it is stored which data node it is stored like a dictionary data dictionary High provide CLI to write High queries using H quy you all have done the assignment four right for H how many of you have done H right just a basic installation right okay High versus traditional relation databases relation databases are a schema on read and schema on write did I explain you what is schema last time like structure dat so when building a relational database like Oracle SQL Server any database which you use then it is schema on read and schema on write means the structures are dependent on schema which is designed for reading and writing okay but when you are dealing with spark and height that is schema on read only because your data is resding in memory and in the desk but those data are not traditional relational database they can be split across multiple data nodes right so you you need the schema to be on read only how you reading the data rather than how you're writing it because writing is pretty simple when you write into the hdfs file system you simply write put statement put will simply write into Data different data nodes right python SQL comes with module that is used to interact with rdbms before you interact with any database through python SQL Library you have to connect to the database connectivity can be established using python ODC method you can do the lab 8 everyone has done you have not done it right I I will okay I will anyways I will show you this method how to do it I will give you the solution of how to connect to the data I'm not going with all this Theory any question you have to ask here please ask here this is features of P spark P spark is one of the utility to be used I you spoke about Li evaluation one time what's the concept I explain you lazy evaluation lazy evaluation means you create structure but you will not execute it you wait till the an event call it then the event transformation and action right transformation is already when you transform the data set which you want to do the computation on but when it is actually to be executed then the evaluation occurs synon any other question here people you all have to read you have to spend some time right please please please read it otherwise it is very interesting topic to be follow it very easy okay this is all about P spk I'm not going to go over this you all can read it it's pretty simple I would like to show you something okay so this is exercise we are going to do it and I will show you how to do it okay what we are going to do is we are going to create a data set first okay fine we going to create some data set and then we'll create some data frame clear and once we are going to create data frames then we are going to import a huge file from a source and then show it in the data frame how it looks like okay everyone understood what I'm trying to do yeah okay you will have to go to since we have not whenever you are doing this uh Big Data operation right Lab One lab two lab three lab four you have all done on your local machine right now you're seeing I'm giving you assignments to be done through Jupiter or notepad jupter notepad or Google collab right why I'm telling you to do it because now the processing capability needs high powered machine right and if you do it on your local machine I'm not saying you cannot do it you can do it but you the the results will not be so uh robust to you to understand it okay so what happens is and what you will be doing in organization is the same way how I'm doing it so in the when you go to an organization you are going to basically given an interface okay you are not going to install spark on your machine you will not be installing Hadoop on your machine right you understand right if you ask me my Hado is not installed on my office machine right where it is there CL it's in the cloud my data is sitting somewhere in the cloud right correct if I have to do the processing and execution on that data what should I do I will be given some interface right I should be able to log in somewhere right once I log in I should be able to connect to that data right once I get connected then I can use that interface to to run whatever I want yeah right when I'm saying that interface which where I'm connecting that interface should provide me high capability in terms of CPU and memory right I'm not doing anything on my local laptop right because my local laptop cannot handle 50 gigs of data right so everything when you do in Enterprise or when you do in a big organization you are going to do it somewhere in the environment where the execution will happen it will never happen on your local machine okay so what we are going to do is we will use Google collab you can use other also there are many other tools in Industry wise you can use data braks you can use Google collab you notebook anything you want okay what benefit you get from that the benefit you get from that is you don't spend time in setting up the environment right in the office if I tell you can you work on this SP program then you oh to your oh it will take 2 days for me to set up the environment then the environment variable oh I don't have an access to change the environment variable because you will not get admin right right they will not give you Administration right to install anything right that is the reason whenever you work in a big organization you do everything in the cloud because tomorrow if you leave the organization you'll take away everything right yes sir we don't want that right we want your work to with us right so similar to that Google CL PAB there are many other utilities which are used across the Enterprise one is called as data braks other is called as Google Google collab is not used because it's a again it may be used in some companies but there are many softwares that are used okay since it's our class so I'm just uh showing you one of the example okay everyone has got Gmail ID right yeah very good so now we will what we will do is we will uh take the help of Google's uh in the right hand side you I already showed you right so what it is telling me so whenever you connect it right so it's connecting right so it connected view resources so if you go to view resources right so it's a free right so they are giving me you connect there huh I don't have to connect it auto connects this m is saying it's giving me connects okay nice connected yeah so you click conect no you don't have to it will auto connect so here you go and you click on view resources so you will see that you are given a machine a virtual machine in the environment which is 12 gigs or 14 gigs whatever it is and a this space of 108 gigs free right right but the free work doesn't work in organization right so when we buy any particular tool they give us a license right so if you go here and you click on man upgrade to Google you see right so organization will use this one pay of what you use right if you see here here you will get machines which are of GPU capacity high power super computers right you can get very powerful machines on the cloud right to execute it depending upon the organization what license they bought it if every tool will have like this if you take data brakes if you take silver core if you take Stone if you take Google collab if you take Amazon Cloud every applic uh every Enterprise buy based on the cost and the usage pattern right so since we are using SLE free right so I don't have to BU my work is sufficient to be performed in a 14 gigs or 12 gigs of time right clear yeah close this oh God where is my [Music] code trying to uh pull out my old one which I did for other people oh yeah I'm just seeing file open resent right I think I got 17 yeah this is only right okay can you all see my screen yes okay so if you see in the bottom it is showing it's not connected right I don't know whether you can see it here right you see it's not connected right now so I can go and click on connect as you were saying right it gets connected it turns into Amber or green the right hand side see now turns into green I don't know how to show you okay you are going to do the similar way when you go in organization okay interface might get changed you might not get the same options right maybe there will be there will be a utility which is an Exe on your machine let double click it it opens screen like this maybe most likely it will be like a browser like this they will give you a Ur L and they will say Okay login with your user ID and password when you log in they will on the left hand side there will be a schema of selecting which nodes you want to connect and all those stuff right which is only one time to be known okay depending on your access okay what we are going to do we are going to first create an environment which has got python spark inside it because I need spark right to run those commands right so what I'm doing is if you see I'm importing SP library from pyspark.sql import all the libraries okay then from ppar the library which I imported do shell I need a command line shell interface import spark I need to import spark Library within that P Spark from pyspark.sql import SQL context if you remember SQL context is an entry point point of Spar I told you in the last class right it's a place where you enter it don't worry I will send you the code mam okay just try to understand this this so what will change and that's why I'm saying what will change in your way you do it in the organization you might have to import certain libraries which are not written for my case okay those your team will help you okay they will tell you okay we need this Library this Library imported one time right you can ask that with your team member right when you are working when I'm importing this and when I run it what I'm trying to do how many of you can explain me what I'm trying to do here you're trying to set up the environment very good I'm trying to set up an environment in that particular virtual machine that is given as 12 GB of RAM and 108 GBS of dis what I'm not doing it I'm not doing the environment variable setup I'm not downloading Spar I'm not updating the bin folder I'm not using bin U bin utils you understand right you were doing all that in your local machine right yeah this interface is helping me to do what don't worry about wasting time in setting up the environment setting up the environment it sets up the environment if you see it downloads the python version on that particular virtual machine setups all the in virtual machine environment for you set up Spar for you Imports the library which you need to run The Spar clear same thing when it happens in the organization you will do it for hadu same thing you will do it for H you understand the lab assignment is given to you for practice purpose on locally doing it you you will never be allowed in the organization to do a Hardo installation they will never let you the admin will never let you to create a data mode right you will be accessing all those system through interf of the environment like this so once I hit this command what I see I see a spark version I see the spark context UI available here this is UI and the spark context SC variable which is running locally on that host where that host right do I need to worry about that machine no no tomorrow if I need a more powerful machine I can go from the drop down and select it if I'm in the office right I need f processing I can select it right Next Step once you have imported all the files The Next Step what you're going to do is I am creating a table table you understand in the database yes here what we are going to call it as I'm going to call it a table in spark how you are creating the table I give the table name employee equals to table will have what columns table will have columns first name last name email and salary four columns yeah this four column will store what data data where it will store row by row yes so I write employee equals to row each row will have first name last name email and salary clear yeah yeah similarly I'm creating another table called as Department department will have two columns ID and name data will be stored with two Fields right ID and name those will be stored in form of a row one below the other employ will be stored one below the other till this point I'm clear good now what I'm doing I am going to insert the data into the employee table and insert the data into Department table right how do you insert in SQL insert table name values equals to right you used to write right value and then same thing I'm doing I'm creating an object of first employee this example first one which we are doing we are going to do with small data set then I will show you to how to do it large data set you don't have to even insert like this this is just to explain you how we insert it like a syntax to let you know so first employ employee EMP equals to table name employee what field I have prer four columns data first name last name email and salary right second employee emp2 employee first name last name email clear yeah 1 2 3 4 5 6 six data where I inserted employ the employee table in the employee table okay so if IUN this if I do print employee five what should it give me employee should give me this one so first I create a table Yeah joh and then you created it see it's created in 0 second right in micro second it might have created now I'm doing the first one it means you can create those two table at the same time don't need to each one okay you can create 100 table that are same same okay see I I inserted the data also right again my table is very simple right it has got only four column that's why it is taking less time if my table can have 50 column it will still take less time okay you can see the processing capability by clicking here like how much utilization you're using it if I click on print five what happens it prints me the fifth name of the record clear yeah clear yes okay now I'm if I put print here if you see I printed with the object name right this name right if I print and if I give the employee table an index if you see 0 1 2 and three it prints me the column name okay you see this one basically the thing which I'm trying to explain you here in spark the column name starts with zero this is zero this is one this is two this is three clear that's the reason I had that print statement written here if you see employee zero should print the first name employee one should print the second name employee two should prepare the third column that is email and employee three will publish the salary that's the fourth column okay it starts with zero clear yes okay now I'm inserting the data in the department table so I call the object Department one Department what two Valu should be passed ID and the name idid and Department name so ID is 111 HR department 2 I give 222 custom Department 3 333 it Department 4 44 logistic right the department table has only two column each column the data has been inserted right I executed it now I print the department third what should it print it should print this one yes right now now see what I'm trying to do first of all try to understand where this is being executed this is executing in the desk or it is executing in M memory memory correct it is executing in the main memory right so I can play around with the data right I can I can manipulate the data I can play around I can create relationship between the table I have two table one is employee and another is Department right yeah I can join both the tables right I can join the table so what I did here Department with employee one so I created row row because it's a logical view it's creating a view basically by joining both the am I physically joining the table no I'm just taking the data from two sources one is employee other is Department I'm going to merge together with the relationship and I'm saying Department with employee one row Department equals to Department one Department one was whatever HR right comma employee equals to employee 1 2 3 so all the employee from 1 2 3 should go into Department one Department with employee two Department equal to two put employee four there I hope you all can understand so look here guys how manys I have enter six right how many rows were there okay first first name last name salary employee email and salary right yes this is which table this is employee right and this is Department table right it has got two right ID and name right 1 1 1 222 333 and 444 right right yeah a b c d right there are two tables right right what I'm trying to do I'm going to join both the table right so this table has let's say first name I'm just writing something right this has data right correct yes what I'm trying to do join both the table so I'm saying ID equal to not this one take this ID one and take two records here and join it what should I get ID equals to 1 how many employee will be there X and C clear I take ID equal to 222 right ID equal to 222 mhm and have employees three and four I'm doing the same thing right I'm taking each department and pushing employee into the department category right yeah yeah what I'm basically doing I'm trying to join the tabl right Department one will have employee 1 2 3 Department two will have employee five Department three will have employee six Department four will employ 7 8 n right dep depart yes so when I do this right I joined it right now I want to see how the relationship looks to me in form of data frame so how I'm going to do is Department with employees here this is a variable name okay equals to Department with employee one the first one it will have what department one with employee three employees inside it Department with employee one and Department with employee two so let's say now the next is I'm going to merge this two to see it in a single place I want to see this ID of employee and this ID of the employee in one place like a data frame so then I created a data frame equals to spark create data frame I'm creating a data frame which is a combination of two of the Department's employees when I created the data frame I simply do display data frame it tells me what is the structure of the data frame it tells me the form of a structure right and then if I do print data frame it will print the data this is display data frame it will tell me structure if I change display and put print data frame it will print the data inside need to do the tables yes I will show you the actual data this is a simple template right so if you change the display to print data frame to print the data which is stored inside it of these two values okay yeah clear simple example let's move to the tougher example so I'm running this I running this okay I don't it takes a little bit time okay okay now this is the practice thing which you will do today on home okay there is a site called as Kegel Hub anyone knows about Kegel okay is a website where you can solve machine learning program and get money out of it nice so they give lot of problem if you want to solve it so ke if you go to this is data science company okay they post lot of questions right people will have lot of competition if you see there is competition right so you can solve the problem and you can provide a m machine learning program to them and then they will pay you also okay people post the problem statement also and they say whoever want to solve it will pay you okay so it's a good practice if you don't have so much skill you can practice it once you learn machine learning okay so I will use this website this website has got lot of data set if you click here you'll see there's lot of data set right movie data set right Picasso data set Global Fashion retail they have raw data right when I talk about raw data means in the office also you will get the raw data huge so there is something called as FIFA data set in only okay I'm directly going to that link FIFA you understand right if you go to this FIFA data set you will find there is a button called is download okay this data set has collected all the data for last 10 15 years of all the players which have played football okay so let me show you how the data set looks like if I download here and download the data set right on download download okay so it see downloading right you all can see my screen right you see this this is archive doz right if I unzip it you don't have to do this right I'm just showing you the samp data right so if I unzip it you see how much data volume is there there are data which is 25 MB 3 MB right so let's take an example let's say I take an example I'll show you how the sample data looks like okay it is a comma separated file CSV right let me take this one it is 13 M it's very biger time to open let me open the smaller one this is okay let me open this one if it opens you all can see the data set right yeah can you see so it has how many columns this is column one 2 3 4 5 6 wow how many columns are there then more than 150 columns right your data can be like this also right your organization data will be like this only so this is a data set of player 20 year 2022 for for all the football player who are registered in FIFA how many what's the age which country they played how many goals they did how many goals they uh they they were supporting like which country they play which Jersey which club they play and all those information is stored in a data file right you can see this right yeah now I want to see this in a data frame so that I can use it for machine learning let's say there is a question asked like for this is the football FIFA World Cup I want to know which which player in this list I'm just giving a simp which player in the list have done a goal against a player Y in a year 2019 2018 where goal was greater than 10 it's a complex query right and how to write it because if I run SQL over that's a huge data right this has how many record let me see this has got it has got how much 19240 records it will take time to process it right if so how to do it what I'm going to do is I will import this data so where is this data kept in the website in the in the cloud right I have to bring the data in the office also you will do the first step in The Spar is to bring the data where it is residing if it is residing in Hado hdfs file system you have to bring it from the data if it is residing in Amazon Cloud you have to bring it if it is an S3 you have to bring it if it is network drive you have to bring it first step is to to import that file right so what you are going to do is in this example if you click on download it gives you some link here can you see this yeah it says import keegle Hub they have a library for it it says get the path where the file is stored and print the part right yeah in your organ ization you will be given this path do you understand you will be given the path where to read the file and they will tell you which Library you have to use to import if it is Amazon web service they will give you the library name if it is uh Google Cloud they will give you some file name right clear so what I'm going to do is I'm going to copy this entire thing because I don't want to remember the path right who remembers the path right I will use it in the data frame if you see here I used exactly the same code here right import Kel Hub path equals to Kel Hub means the site do data set underscore download means bring it from where from that website path where the data is sitting right and since I want to when it is pulling the data I need to know the path give me the location so what I'm saying print path of the data set file path so this variable path which will path where the data is read will give me the path here and will print it for me because I can use that path to manipulate the data then clear so if I run this see what it does it will import the library which is required and it will go to that path and it will read the and let me know the print out the path where the data is deciding are you bringing this data on your local laptop no no where it is this data the cloud where it is running the environment which you have created right everything is happening there right nothing is on your machine right now I will I know the path right path is this one right when I unzip that file you saw multiple CSV file right you don't need all the file right let's say there are hundreds of files you you need 1 2 5 10 to play around right so what I do is I have the path this is the path if you see here to here right here to here is a path when I unzip it there are multiple CSV inside it I took one player 22. CSV understood yeah so when in the organization when you connect to the particular data source where the data is to be pulled out there will be hundreds of files right correct you don't need all 100 files you may need five 10 15 whatever you need whichever file you need you give this path so how you pull the data this one is downloaded in your environment now you want to pull the data to view it this is variable name FIFA data frame you can give any name equals to spark do read do format spark. read will what what it will do it will read the CSV correct it will read the Cs before before you continue the the command languages are they character sensitive no no you can write so spark. read. format what are you telling you're telling Spar to read form in what format the file is existing is it text file txt CSV bit map blog whatever jpg you have to tell them format right I told them it is a CSV file Dot option in the parenthesis header equals to True basically means the first line of data set in the CSV did you see the first line data set let me show you if I open this file right this was this line what is this line is it data or is it only the name of the column name I have to tell the system right this is not the data it's a header name right they are made situation where the header name is not there then you have to start reading from the first line itself right here the data has got header so I have to tell when you read it read the first line as a header not as a data right so how to read that I tell the system spark do option header equal to true if the first line was not a header it starts with the data itself then you will write header equal to false false clear dot option in in for schema equals infer schema true infer schema means in whatever format and data type data is resding keep it like that only if it is integer keep it integer if it is character keep it character if it is numeric keep it don't change the data type because if the data type has been changed you cannot use the data maybe you want to do let's say data is in float with four decimal places if you don't put in first schema equal to true what will happen maybe data will be converted into two decimal so you lose the remaining decimal places value right if you write INF first schema equal to false it will convert into own format so better always and always do infer schema equal to true you don't want to change the structure data type of the data you don't want to change the schema of the data then only you can be sure that the data you are viewing is true right dot load do load path what is the path path where my data file is kept so path you see here you got the path right root do if I take this path and put it in a variable called as path and use that in the path there or you can directly write do load and give the entire path here to here player. CSV you can do that also why this path is given because we just want to shorten the name right who will write that big name right every time how why will you write that full path every time so let's say this path has got 200 file inside it everywhere you will not write load and full path SL then CSV SL then CSV right rather you create One path variable and then every time you change the file name after that like player. 222 player so these parts now you name it parts and then you define it then you use it if I can I choose to use another name to say yeah you can use any name this is variable name you can use any name a b c x y z whatever now what has happened here my data has been pulled from the source and data set has been created data set has been created now if you do FIFA dataframe do show then the value which is coming in form of a table is called as data frame now you see the structure you see this structure right this is data frame anything before this is data set clear now I can see the data frame if you if you see if I run this run this nothing happens it just pulls it it's pulling it it's creating data set but it will not give me any output it took 8 seconds it shouldn't take that long okay and then when you go go and click on data frame now the displays in the form of a table and column clear now if you see if I scroll it down right you see all the columns right clear now what is the advantage of seeing it like this source is it's easier right first of all it's in memory now it's not in the desk it's not in the location in hdfs it's on my local in memory right now if you see only showing top 20 rows because if you start showing all 2 19270 rows then your main memory will be occupied with a whole bunch of data it's already occupied but it is only showing you what is relevant to you if you want to see any specific row like for example when we'll do the machine learning I want to see which column is zero then we will write some code below there 119 Cod okay you want to see all the all the what the can't see no you cannot you can see it but if you do data frame do show and you give th okay what the point of seeing that right no I just want to know the default on how by default okay now if I do data frame this is the again data frame do count it will tell you how many rows of records are there it's like SQL statement select count star from table right it will give you the number of count right now if you do data frame dot everything now you will use the variable name which you created data frame everything this is the point which will be used to do any operations so if I do tia. dataor dataframe do columns it will list out all the columns in the table right see it gives you all the colum right correct oh sorry please go if you class please practice it I will send you this code in the word document thank you everyone in the call