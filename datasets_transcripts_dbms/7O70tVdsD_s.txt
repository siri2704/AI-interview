hey everyone and welcome to the Microsoft Azure data fundamentals virtual training day I'm so excited to have you all here and I hope you're ready to dive into the amazing world of data with me I'm Christa Santos your Microsoft technical trainer for this journey Over the next few sessions we're going to break down some super important concepts around data and the Azure services that bring it all together Now my main focus for this data fundamentals virtual training day is simple I want you to walk away feeling confident about data What it is how it works and how you can use Azure to manage it But don't stress about becoming an expert right now Today is all about understanding the why and the what behind these technologies Plus we're going to get hands-on with them so you'll actually see how everything works in real time Let's take a look at what we have planned for this data fundamentals virtual training day We have six exciting sessions planned for you We'll kick things off with sessions 1 2 and three where we'll break down the core data concepts you'll need to know focusing on relational and non-reational data in Azure Once we've got the core concepts down we'll move into sessions four five and six This is where we will dive into largecale data analytics real time analytics and data visualizations By the end you'll have a solid grasp of how to transform raw data into valuable insights All right enough intro Let's dive into session one and kick things off by exploring the fundamentals of data What is data why does it matter so much and how does Azure come into play when managing it let's get started and find out All right let's jump right into some core data concepts by tackling a seemingly simple but big question What exactly is data data is the heart and soul of every information system is the fuel that powers everything we do with technology Whether it's numbers descriptions or observations data is how we capture store and make sense of the world around us But here's the thing Data isn't just some random collection of facts floating around It's carefully organized into structures that represent meaningful elements or entities that are crucial to any business or organization Think of data like a puzzle Each piece represents an entity And when you fit those pieces together you get a complete picture of what's going on in the business And just like in a puzzle each piece each entity has specific details that define it When I say entities I'm talking about the building blocks of an organization These could be customers products orders whatever is core to the business Each entity has attributes which are characteristics that describe it For example if we're talking about a customer their attributes might include their name their address phone number Basically all the details that help us know who they are Similarly for an order you might track the order date the total amount the customer who placed it And hey without these attributes working with data would be like trying to put together a puzzle without knowing what the picture is It's possible but it's going to take a lot of guesswork By organizing data into these entities and their attributes it's like putting together that puzzle Businesses get a clear complete view of their operations This structure allows them to understand how everything fits together make sense of complex relationships and make faster and smarter decisions Now that we've got a handle on what data is let's talk about the different flavors it comes in Generally data can be grouped into three main categories: structured semistructured and unstructured Think of it like organizing your closet Some things are neatly folded others are a bit mixed up and then there is a pile of stuff you're not quite sure what to do with Structured data is the most organized form of data It follows a consistent format that makes it easy to store retrieve and manage Think of structured data as a well organized system where everything is neatly arranged according to a predefined pattern like a filing cabinet In this filing cabinet each drawer represents a different category of information And within each drawer folders are labeled clearly making it simple to find exactly what you need For example imagine a filing cabinet in an office where each drawer represents a specific type of record customers orders or products Within the customer drawer each folder contains the information of an individual customer neatly separated by categories like name address and phone number This setup ensures that every piece of information is stored in a predefined format so that when you need to look up the details about a customer you know exactly where to go and what to expect Now say you want to find all orders placed by a specific customer You'd first pull up the orders drawer and find the relevant folders which are linked back to the customer's details using a unique identifier like a customer ID number The filing cabinet analogy shows how structured data keeps everything neatly organized allowing you to retrieve information quickly and efficiently In the digital world structured data operates just like this filing cabinet Every bit of data fits into rows and columns just as it fits into folders This structure makes it incredibly easy to search sort and analyze data with minimal effort Whether you're handling thousands of customer records or managing inventory for a retail store the structured approach ensures that everything is in its right place ready to be accessed when needed just like flipping through well-labeled folders in a perfectly arranged filing system Now let's move on to semistructured data Unlike structured data which fits neatly into predefined rows and columns semistructured data doesn't follow a strict format but still retains some level of organization Think of it like a filing cabinet where folders aren't always organized the same way but there's enough structure to find what you need A great example of stream data is a JSON file commonly used in web applications Imagine opening a folder labeled customers in our filing cabinet For one customer the folder might contain their name two phone numbers and an email address For another customer the folder might have a name a single phone number while a third folder might be missing a phone number entirely Despite these differences everything is still stored in a way that's organized and easy to retrieve The beauty of semistructured data is its flexibility It can adapt to varying amounts of information without needing every record to conform to the same format This is especially useful in situations where structure of the data is constantly changing For example an e-commerce platform might use JSON files to store customer details product cataloges and transaction histories One customer might have several orders and multiple shipping addresses while another customer might have made only one purchase and provided a single address In this case our filing cabinet allows for different folders with different contents but there's still enough structure to keep things manageable The adaptability of semistructured data means it can accommodate both standard fields and optional ones making it ideal for dynamic environments where data evolves over time Lastly we have unstructured data The most free form type of data with no predefined format or structure If we continue with the filing cabinet analogy imagine you've got a drawer full of random items loose papers photos videos handwritten notes and even voice recordings Nothing fits into neatly labeled folders or follows a standard template Yet each item holds valuable information Unstructured data can take many forms such as text documents emails images videos or audio files Picture a digital archive filled with product photos customer reviews and video testimonials These may not follow a structured format but they still provide important insights For instance a video testimonial from a customer or a product review can tell you a lot about the customer's satisfaction and preference Social media posts email records and customer support chat logs are also examples of unstructured data The challenge with unstructured data is that analyzing it requires advanced techniques For example to make sense of textbased data like emails or social media comments we might use natural language processing Similarly image recognition technology helps analyze visual content such as photos and videos While unstructured data is more complex to work with it often contains a wealth of insights that when properly analyzed can drive smarter business decisions Now that we've explored the different types of data it's time to dive into how this information is stored Depending on the type of data whether it's structured semistructured or unstructured there are specific storage solutions that are best suited for each However there are also versatile storage systems that can accommodate more than one data type For structured data one of the simplest storage methods is delimited text files like CSV files These files arrange data in rows and columns making it easier to read and manipulate For example an inventory management system might use a CSV file to track product names and prices and stock levels But as data becomes more complex or grows in volume CSV files may not be enough That's when other advanced storage systems come into play For semistructured data JSON is one of the most common formats JSON is lightweight and easy to use especially in web applications For instance if you're building a web app that collects customer data JSON would allow you to store this information in a flexible format supporting different numbers of attributes for each customer JSON is also commonly used in APIs to exchange data between between systems making it a versatile choice for many modern applications Another format you might encounter is XML which is particularly useful in industries that require strict data formatting such as finance or healthcare XML allows for hierarchical data storage with tags that describe the data making it useful for scenarios where data integrity and structure are paramount For instance healthcare organizations often use XML to store and transfer patient records between systems ensuring that data is easily readable across different platforms Now when working with large multimedia files such as images videos or audio you'll often use blobs which stand for binary large objects Blob storage is specifically designed to handle massive files that can't be easily stored in traditional relational databases Think of it as a space for all those highquality videos photos or audio files that need to be accessed quickly without sacrificing quality For example imagine a video streaming service that stores movies and TV shows These files would be stored as blobs allowing the service to deliver content seamlessly while ensuring the data is available in the highest possible quality even at large scales Blob storage is like the ultimate digital vault for all those giant media files ready for fast retrieval whenever you need them for handling massive data sets especially in big data environments Optimized formats such as Avro OC and Parquet are your go-to options AVO is a row-based format excellent for data serialization and storage particularly when systems need to exchange data because it includes both the data and its schema This makes it highly efficient for writing data or serializing it for fast transmission between systems Parquet on the other hand uses a columner storage format where data is stored in columns instead of rows This makes parkquet especially powerful for analytical workloads where you need to query specific columns rather than the entire data set Lastly OC or optimized row columnar is also a columnar format like parquet but often provides better compression and faster query performance for specific types of queries Originally developed for Hadoop OC is widely used in big data applications Next let's talk about databases the backbone of data storage There are two main types of databases relational and non- relational Relational databases are ideal for structured data They organize information into tables with relationships between tables established using keys Think about a banking system where every account transaction and customer is recorded Relational databases ensure that all this information stays accurate consistent and easy to retrieve Their structure makes them especially reliable in scenarios where consistency is critical Now non- relational databases or NoSQL databases offer much more flexibility Instead of tables they store data in various formats including documents key value pairs and graphs Graph databases are particularly powerful when dealing with highly interconnected data A great example is a recommendation engine Think of how streaming services or online retailers suggest movies music or products that you might like based on your preferences and behavior Graph databases map out relationships between users items and preferences making it easier to recommend new content or products based on those connections Take social media platforms for instance Users post text upload photos share videos leave comments all of which are unstructured or semi-structured data Non- relational databases excel at handling this kind of varied and interconnected data When we think about how we use data in everyday systems it typically falls into two main categories First we use data to support realtime operations where everything needs to happen instantly Then there is data analysis where we process and analyze large amounts of data for insights and decision making Let's start with realtime operations This is where OOLTP systems or online transaction processing comes into play These systems are designed to handle fast high volume transactions in real time Think about it Every time you place an order online transfer money between accounts or even book a table at a restaurant the system needs to instantly update everything from inventory to payment details to availability That's what OLTP systems are built for ensuring that transactions happen quickly and accurately What makes OLTP system so reliable are the asset principles A set of rules that ensure every transaction is handled safely and efficiently no matter how fast things are moving Let's break them down First there is atomicity This means that every transaction must be fully completed or not completed at all Imagine transferring money between two accounts Both accounts need to update at the same time If something goes wrong the entire transaction will fail preventing any partial updates and ensuring consistency Next is consistency which ensures that the database always remains in a valid state Any changes made by a transaction must follow predefined rules and constraints So the data is always accurate and trustworthy Then we have isolation Even if multiple transactions happen at the same time they don't interfere with each other For example if two users are trying to update the same record simultaneously the system keeps those transactions separate to avoid any conflicts or errors Finally durability guarantees that once a transaction is committed it's permanent So even if there's a system crash or power outage the completed transaction will still be there when the system comes back online Now let's walk through the process of how operational data moves from raw form to being used in analysis and decision making Imagine you're running an e-commerce business Every day your systems are collecting data on customer orders website interactions and inventory levels This is operational data raw unstructured and coming in from multiple sources First this operational data is extracted transformed and loaded into a data lake The data lake serves as a central hub where all this raw data is collected and stored for future analysis Next the data is organized into a schema of tables This can happen in two ways Either through a Sparkbased data lakehouse where data is stored in files but accessed through tabular abstractions or in a data warehouse which uses relational SQL engine to store and manage data in a more structured and tablebased format For example data about customer purchases product details and sales figures can be neatly organized here Once the data is in tables it can be aggregated and loaded into an OLAP model or cube which allows for powerful efficient analysis This step is crucial when you want to understand trends over time For instance you might analyze how sales in one region compared to another or how a specific product category has performed over the last year Finally whether you're working with files in the data lake relational tables or an OLAP cube all this data can be queried to produce reports and dashboards These tools give you a real-time view of your business performance such as customer behavior top selling products and even inventory forecasts This is the data that helps inform strategic decisions like adjusting marketing campaigns or optimizing stock levels So let's move into our next lesson in this module Data roles and services In the data world there are many roles each playing a vital part in managing and analyzing data While we won't cover every role here like data architects who design the overall architecture of data systems or data scientists who focus on advanced analytics We'll focus on three of the most common and essential roles today First up we have the database administrators or DBAs These professionals are responsible for ensuring databases run smoothly They monitor performance manage database security and ensure proper backups are in place DBAs are also tasked with managing user permissions and making sure that sensitive data is protected from unauthorized access For instance in a financial institution a DBA might ensure that the customer account data is encrypted and only accessible to authorized personnel maintaining the highest level of data security Next we have data engineers Data engineers are the builders of the data pipeline They design create and maintain systems that collect store and process data A key responsibility for data engineers is handling the ETL process which stands for extract transform and load This involves extracting data from various sources cleaning and transforming it into a usable format and loading it into databases or data warehouse For example a data engineer in healthcare might create a pipeline that extracts patient data from multiple hospitals standardizes it and loads it into a centralized database This ensures that all patient data is ready for analysis and easily accessible Finally we have the data analysts The data analysts play a crucial role in transforming raw data into actionable insights One of their key responsibilities is data modeling which involves structuring and organizing data in ways that make it easier to analyze They also create reports dashboards and visualizations that help businesses understand their data For instance a data analyst in a marketing department might model customer data to understand which products are performing best among different demographics Using this analysis they can help company make more informed decisions about marketing strategies and product development As we wrap up module one let's briefly touch upon some of the Microsoft cloud services that can help you manage store and analyze data For operational workloads we have services such as Azure SQL and Azure Cosmos DB Azure SQL is a fully managed relational database service that offers high availability and automated backups keeping your data secure and always accessible Azure Cosmos DB on the other hand is a globally distributed NoSQL database built for high performance and scalability making it perfect for applications that require fast realtime data access across multiple regions especially when dealing with diverse data models On the analytical side tools like Microsoft Fabric provide a unified platform for integrating multiple data sources and performing analytics without heavy infrastructure And for advanced big data processing and machine learning Azure datab bricks offers a collaborative environment that excels at building complex data pipelines and running sophisticated models across large data sets Of course this list isn't exhaustive There are many more powerful tools and resources within the Microsoft Cloud ecosystem and we'll be diving deeper into those in the upcoming modules And with that we've officially wrapped up session one We've covered the basics of data classification explore different types of data storage and data workloads and discuss the key roles involved in data operations Now let's take a 10-minut break and take this opportunity to stretch recharge grab a quick drink or snack When you come back we'll dive into our second session where we will explore relational data in Azure See you soon Hey [Music] Hey [Music] wo wo Oh Oh Oh [Music] Oh Oh Hey Woah Woah [Music] Oh hey [Music] [Music] Hey Hey Hey Hey hey hey [Music] [Music] Hey Hey Hey [Music] Heat Heat Heat Heat [Music] [Music] [Music] Hey hey hey [Music] Heat Heat N [Music] [Applause] [Music] [Applause] [Music] Hey hey hey [Music] [Applause] [Music] the heat [Music] [Applause] [Music] Heat Heat [Music] [Music] Welcome back I hope you've had a chance to stretch recharge and maybe grab a quick snack Hopefully you didn't get pulled into any work during your break In session one we laid the foundation of our data journey covering the essentials of data management key roles and Microsoft Cloud Services Now we're ready to kick things up a notch as we dive into session two where we'll explore the fascinating world of relational data in Azure We're going to break down the core principles of structured data and databases and cover the power of relationships within data This will give us a solid understanding of how to effectively organize and manage data in a way that drives smarter decisions Let's kick things off by getting familiar with relational tables One of the core pieces that makes relational databases so powerful In a relational database data is organized in tables which you can think of as spreadsheets These tables are made up of rows and columns Each row holds a unique piece of information like a customer a product or an order And every row in the table follows the same column structure These columns define the attributes or characteristics of the data For example in a customer table you'd have columns for the first name last name and email address Now here's the flexible part Not every column needs to have a value Let's say there's a middle name column for our customer table It's totally fine for some rows to leave that blank or to have what we call a null value meaning there's simply no data provided for that particular piece of information Each column is also designed to handle a specific type of data So for example an email column would store text which could vary in length depending on the email address On the other hand a price column in a table would store decimal values for currency while a quantity column in an order table would be set to store integer values And we can't forget about the date and time An order date column in the order table would be designed to store datetime values making it easy to track when orders were placed It's worth mentioning that different database systems may have slight variations in data types they support but most systems follow a standard set of data types established by the American National Standards Institute also known as ANI So no matter what system you're working with you can rely on a pretty consistent set of rules when defining your tables Now I want to shift our focus to something crucial in the world of relational databases Normalization Think of it like mastering the art of organizing your data for maximum efficiency It's all about cutting down on the clutter and making sure your data is crystal clear and accurate Let's start with a simple example Imagine a company is using a spreadsheet to track its sales And in that sheet you see customer details product information and prices all repeated for every single sale The same customer's name and address appear over and over again alongside the product names and prices It's messy repetitive and leaves a lot of room for mistakes This is where normalization comes in When we normalize data we transform that chaotic spreadsheet into a well organized system Instead of stuffing everything into one table we break the data down into smaller more focused tables For instance we'd have a separate table for customers products and sales orders and line items Each table would store just the relevant details like the customer's name the product's price and so on But here's the real magic Each of these tables is tied together using something called a primary key Think of it like a unique ID for each entry So instead of repeating a customer's details for every order we store the customer's ID in the sales order table And that ID links in back to the customer's table where their full details are stored only once Need to know a customer's address for an order no problem Just look it up using their customer ID This approach keeps your data clean organized and efficient One more important thing referential integrity It's like a built-in safety net for your database Whenever you enter a foreign key say the customer ID in an order it has to match an existing primary key in the customer table This prevents mistakes like trying to place an order for a customer that doesn't exist Now that we've got a solid understanding of how relational tables are structured and how we optimize them through normalization let's dive into the fun part How we actually interact with relational databases The language we use for this it's called SQL or structured query language But SQL isn't just any language It's the universal communicator for your data Think of it like a common language that all relational databases speak And here's what makes SQL even more impressive It follows a strict syntax set by the American National Standards Institute ANC and the International Standards Organizations ISO This means that no matter what database you're working with SQL helps ensure consistency and compatibility But there's a twist While SQL provides a strong foundation many relational databases have their own unique flavor of SQL adding specific features and syntax For example you might have heard of TSQL or transact SQL for Microsoft SQL Server PL SQL for or O or O or O or O or O or O or O or O or O or O or O or O or O or O or O or O or O or O or O or O oracle or PGSQL for Postgress SQL Each of these has its own little tweaks but at the core they are all built on standard SQL Now you don't have to worry about all those variations right now Let's focus on the three common types of SQL statements you'll encounter each serving a distinct purpose in managing your database First up we have the data definition language or DDL This set of statements is all about managing the objects in your database like tables or any other object With DDL you can create alter or drop tables For instance when you use something like create table statement like the example on the slide you're creating a new table called products Next we have the data control language or DCL This is all about controlling who has access to your data With DCL you can grant deny or revoke permissions for users or groups For example you could give user one permission to select insert and update data in the products table And finally we get to the data manipulation language or DML This is where SQL really shines It's all about interacting with your data With DML you can insert update or delete specific data in your tables For instance the example on the slide shows a select query that pulls product names and prices based on a specific condition Now keep in mind SQL is a massive language We've just scratched the surface here If you find SQL intriguing and you want to dive deeper there are some great resources out there like the DP080 course on quering data with Microsoft Transact SQL or the get started with quering with transact SQL learning path on Microsoft Learn But the key takeaway here SQL is your go-to tool for managing accessing and manipulating relational data Once you master it you unlock the full potential of working with relational databases Tables aren't the only important objects you'll encounter in a relational database There are other few key players that help make everything work smoothly like views store procedures and indexes Let's start with views Think of these as your predefined SQL queries that return structured data sets almost like virtual tables You can query them using select statements just like you would any other table Views are super helpful when you want to simplify complex queries or hide the complexity of underlying database structure It's like having a shortcut that lets you focus on the data you need without worrying about the nitty-gritty details Next up we have the stored procedures These are defined SQL commands that can be run on demand almost like many programs in your database What's cool about these stored procedures is that they allow you to add parameters for flexibility in your data operations like inserting updating or deleting records across your data entities They make repetitive tasks easier and more efficient allowing you to run complex data operations with a single call And then we have our indexes Think of these as the navigation guides for your database They act like a road map for your query engine helping it quickly find the data it's looking for based on specific column values By speeding up data retrieval indexes make your database operations more efficient especially when dealing with large data sets Together views store procedures and indexes and other objects all work behind the scenes to create a robust and efficient relational database They empower your applications to securely manage store and retrieve data with ease and they play a crucial role in how we interact with and manage our data Now that we've covered the concepts behind relational databases it's time to see them in action This is the moment you've all been waiting for Let's dive into some of the Azure resources designed specifically for relational data First up we have the original Azure SQL Azure SQL is a family of services built on the solid foundation of Microsoft SQL Server one of the leading relational database management systems trusted by some of the world's largest organizations While SQL Server is widely used for on premises solutions Azure SQL brings all that power and reliability to the cloud offering flexible solutions for just about any scenario Now Azure SQL comes in three different flavors each tailored to specific needs First we've got SQL Server on Azure virtual machines This is like having your own virtual database server in the cloud It's perfect for projects where you need full compatibility with existing onremises SQL server setups or for hybrid environments where you're combining both cloud and on premises databases It's the go-to choice if you want full control of SQL Server but in the cloud Next we have Azure SQL managed instances This is a platform as a service or p solution that automates many core administrative tasks making it ideal for migration scenarios If you're looking to move your on premises SQL Server databases to the cloud with minimal changes and without worrying about managing hardware this is a fantastic option It gives you the flexibility of SQL Server but with a lot of heavy lifting taken care of And finally we have Azure SQL database which is another pass solution and probably the most cost effective option out there It's perfect for new applications that need a lowcost relational database with minimal administrative overhead If you're looking for a simple scalable option without needing to worry about managing infrastructure Azure SQL database has you covered Now here's a handy tip to keep in mind The options we've covered are arranged in decreasing order of administrative control and cost SQL Server on a VM gives you the most control but also comes with higher costs On the flip side Azure SQL database offers the least control but it's the most budget friendly or choice But don't think you're limited to just Azure SQL Open-source databases have become a cornerstone of modern technology because they bring a lot to the table Flexibility cost efficiency and innovation And Azure has fully embraced these open-source powerhouses to give you even more options to choose from First up we have MySQL a friendly open-source database management system that's incredibly popular thanks to its simplicity You'll often find MySQL in the LAMP stack applications working alongside Linux Apache and PHP It's a go-to for developers who want a straightforward reliable database solution Next is Maria DB Fun fact Mariab was created by the original developers of MySQL Over time it's been optimized for even better performance making it a strong contender One cool feature of Mariab is its compatibility with Oracle databases offering you even more flexibility in your database management And finally we have Postgress SQL Now Postgress SQL is a bit of a hybrid It's part relational database part object databases What sets it apart is that while you can store data in traditional relational tables Postgress SQL also lets you work with custom data types offering you flexibility that goes beyond conventional databases It's a real gamecher when you need more than just the basics All right we've talked a lot about databases So how about we get some hands-on practice with Azure SQL databases Let's log into the Azour portal and get started All right so here we are in the Microsoft portal And this is where we are going to be doing our demo today In order to create our uh relational um Azure resource we're going to click the create a resource button and we're going to search for Azure SQL Now the first item in the marketplace is going to be the one that we're looking for and the resource we're going to be creating So I'm just going to select the create button there and we're going to be focusing on SQL databases but we also have SQL managed instances and SQL virtual machines but for this demo we'll be focusing on SQL databases The first thing we always do when we create a resource is add it to a resource group And then we're going to give a database name For this database we're going to call it Adventure Works And then we're going to create a new server for this database I'm going to call it data demos We're going to add it to the West Europe region And then we're going to figure out what kind of authentication we want to give this SQL Server database We have three options The Microsoft Entra only authentication both SQL and Microsoft Entra authentication or only SQL authentication For this demo we're going to use the last option use SQL authentication And I'm going to give an admin login here And I'm going to create a password for this server as well Now we have to remember this password because we are going to use it later on in this demo to log this server All right So now that we have created our um server we are going to make sure that our workload environment is development because this is just a demo And also we're going to keep um our compute and storage to the same default option general purpose serverless And for the backup storage redundancy I'm going to shift to the locally redundant backup storage because I do not want to put it in zone or go because that's extra backups which we do not need for this demo So when it comes to the networking part I'm going to select public uh endpoint here um so that uh I can connect from my um browser into this SQL server database If I don't select this then I will need to go through a private endpoint or create a virtual network through which I can get access for security purposes But since this is a demo let's keep it to a public endpoint Next we will go to the firewall rules In the firewall rules I'm going to make sure both of the options here are uh ticked to yes because this is going to allow me to allow Azure services and resource to connect to this server And also I'm going to set up my current IP address to access to be accessible by the server All right So let's go to the next step which is security I'm not going to turn on the Microsoft Defender for SQL I'm just going to keep it to not now And then let's go to the next steps which is additional settings where I am not going to change much here except I'm going to add a sample database in the use existing data part So I'm going to select sample there and it's going to add some data so that we have some data already to work with in our database And with that we are done with the configuration of the SQL database and we can just hit review and create We can see the estimated cost which is uh 5.69 USD per month and also the compute cost for any compute uh resources that we use So we core seconds is the unit we use here So we can see all the settings and configurations we have set up and just hit create So now in this deployment we're actually deploying a SQL database but we're also deploying a SQL server So this is going to take some time All right So here is our resource successfully deployed So let's go to the resource and we can do that by simply clicking the go to resource button All right So here we have the SQL server name and the database name Now in order to query this let's go to the query editor And here I'm going to use the SQL server authentication Remember the password I told you we're going to use later That's where we're going to put in the password to log in uh to this uh SQL server And here we are in the SQL query editor And um I I I can check my tables I have a lot of tables because we prepopulated this with some sample data And what I'm going to do is query that product table that we have here So I'm going to write a very simple select query here So select star from sales lt um product and let's run that and you can see I get the results of this uh table This is structured data right we have column names and we have rows And we have a lot of columns here right and I can scroll all the way up and down to see all these columns all these rows But what if I want to see only a certain set of columns instead of all the columns i can do that by just changing this query right um so you can see I have all of these other columns here And what I can do here is I can just add the column names that I want like product ID name list price and um product category ID I'm just going to put that from into the next line And then I'm going to run this query And now you can see I only have four columns So if I open this up you see I do not have any more columns showing up There's no scroll bar available here Right but instead of product category ID what if I wanted to see the product category name now this information is not available in this table It's available in another normalized table that's available in my database called product category So that's the product category table there And so I'm going to create a join here to join these two tables so I can get that product category information So I'm going to say join join with my product category table I will need to rename these tables so that I can uh reference them easily So I'm going to call the first table P and the second table C And then I'm going to put the condition of my join which is on the column that both of the tables have in column common which is product category ID So P do.t dot product category ID is equal to C do.product category ID I'm just going to copy that over there Now that I've done this the only thing left for this query is to use the reference in the column names because if I don't use it I will not know from which table I need to get these columns So the product ID the name the list price they all come from my first table the product table and my product category ID I'm going to replace But I'm also going to rename the product name column to product name because I'm going to get product name and I'm going to get product category name So that last product category ID is going to be renamed to category So I'm going to call it C.name as category And then I can run this query And now you can see instead of category ID I have the actual name of my category Right so this shows you how you can get that product category data into your product uh table doing a very simple join And these are entities right so these are normalized entities To reduce the redundancy of having the all the category information repeated in our product table we create a separate product category table that is normalized and has each category mentioned only once Instead of each category information showing up multiple times and through a join we can slice and dice this data Slice our product data with our product category data And with that we have come to the end of this demo That was a pretty cool demo of Azure SQL databases What did you think about it hope it makes you curious to test it out yourself With this demo we've come to the end of session two We have learned about relational databases how and why we normalize tables within them and some SQL and other important database objects as well as Azure services and services available for open-source options Great Now it's time for another 10-minute break and I'll see you refreshed and recharged for the third session where we'll see non- relational or NoSQL data in action in Azure See you soon [Music] Hey hey [Music] Woah Woah Woah Woah [Music] Oh Oh Hey [Music] Oh oh oh [Music] Heat Hey Hey Hey [Music] Heat Heat [Music] [Music] [Music] Heat Heat [Music] [Music] [Applause] [Music] Heat Heat [Music] [Applause] [Music] [Applause] [Music] Heat Heat [Music] [Applause] [Music] Heat Heat Heat Heat Heat [Music] [Music] Welcome back everyone I hope you had a refreshing break and are ready to dive into session three In this session we're going to explore the fundamentals of Azure storage which is the backbone of so many cloud solutions But that's not all I'm also excited to introduce you to Azure Cosmos DB which is handsdown one of my favorite Azure data solutions It's a real gamecher when it comes to working with globally distributed higherformance databases But before we get to Azure Cosmos DB let's start with the basics and build a solid understanding of Azure storage Now do you remember what blob stands for from session one we know that blob stands for binary large object And in Azure blob storage we're talking about storing massive amounts of unstructured data or blobs in the cloud Inside your Azure storage account you can create these blobs within containers Think of containers as handy folders where you can organize related blobs kind of like how you organize files on your computer's hard drive Now what types of blobs can you work with in Azure blob storage well there's a few so let's break it down First up we've got block blobs Imagine them as stacks of blocks each capable of holding up to 100 megabytes of data A single block blob can have up to 50,000 blocks which means it can grow to a whooping 4.7 tab These are perfect for storing large unchanging files like videos So if you're uploading cat videos in 4K no judgment block blobs are your friend Next we have got page blobs These are organized into fixeds size 512 byte pages and are optimized for random read and write operations Think of them as your go-to storage for virtual machine discs If you need to run a virtual machine in Azure page blobs are ready to store the data efficiently Then we have append blobs They are a bit like block blobs but with one special feature You can add only to the end This makes them ideal for scenarios where you're constantly adding new data like logging So if you're storing a continuous stream of log files append blobs are the way to go Now storing data is one thing but often you also need to think about how often and how fast you'll need to access that data Azure gives you access to three access tiers to balance speed and cost First up we've got the hot tier This is the default choice for frequently accessed blobs that need to be super responsive Think of it as a topshelf higherformance option If you've got a web app serving up images for users this is where you want your data instantly access accessible with minimal delay Next we have the cool tier It's a little slower but much more cost effective if you don't need the data as often For example if you're generating daily transaction logs that are frequently accessed at first but less so over time you can start them in the hot tier and later migrate them to the cool tier when they're not being used as much It's a perfect way to save costs while keeping your data within reach And finally there's the archived year This is the lowest cost option available but it comes with increased latency so it's best for data you rarely need but must keep safe Think of it like your long-term storage for historical data Stuff you do not need now but you might need for compliance or legal reasons It's there when you need it but not taking up extensive resources in the meantime And if you're anything like me as soon as you hear about these tiers you're probably thinking "Wouldn't it be great if this could just be automated?" Don't worry You can set up life cycle management policies to automatically move your blobs from hot to cool and eventually to archive as they age You can even automate the cleanup process by deleting outdated blobs Pretty sweet right in the end Azure blob storage gives you the flexibility and power you need to efficiently manage your unstructured data no matter how big or how varied it is Talking about Azure blob storage another storage account comes to mind Azure data leak storage This is a powerful service that's essential for managing and analyzing massive amounts of data in the cloud If you're working with big data this is definitely one service you need to know about But before we dive into Gen 2 let's give a shout out to Azure Data Lake Storage Gen 1 Gen 1 was designed specifically for organizing data in analytical data links Imagine a lake filled with all kinds of data structured semistructured and unstructured Gen one helps you keep it all organized and accessible for analytics It was built to be a solid foundation for handling large data sets and making sense of them Now here's where things get exciting Azure Data Link Storage Gen 2 takes it to a whole new level It's integrated directly into Azure storage bringing together the scalability of Azure blob storage and the costsaving features and storage tiers and the hierarchical file system organization It's the best of both worlds solution and it's completely compatible with major analytical systems So what does this look like in the real world let's say you're working with Azure HD Insights a big data analytics service With Azure Data Lake storage gen 2 HD Insights can mount the distributed file system hosted on your data lake and seamlessly process massive amounts of data It's like having a powerful engine ready to drive all your data analysis Or maybe you're using Azure Data Bricks a collaborative analytics platform Data bricks can tap into data lakeink gen 2 to manage and process large data sets efficiently making it an indispensable tool for data scientists working with big data Now I can almost hear you wondering but how do I create a data lake with Azure data lake storage gen 2 don't worry I've got you covered It all starts by enabling the hierarchical namespace option in an Azure storage account You can either do this when you're creating a new storage account or you can upgrade to an existing one to support data lakeink gen 2 Just remember the upgrade is one way So make sure to plan carefully before making that change Whether you're managing vast data sets or powering cuttingedge analytics Azure data lakeink storage gen 2 is an invaluable tool is built to handle scale and complexity of today's data demands and help you unlock the full potential of your data Next up let's talk about Azure Files And trust me if you're familiar with traditional network shares you're going to love this one At its core Azure Files mirrors that classic network share concept that you typically find in an on premises setup It's all about making files and documents easily accessible to multiple users But here's the twist No more hardware No more maintenance headaches Seriously kiss those costs goodbye Now picture this You've got users working across different operating systems Windows Linux and even Mac And you might be thinking how does that work in the cloud well with Azure Files it's a breeze It provides seamless SMB file sharing So whether your users are on Windows or Linux they can access files effortlessly just like they would in a traditional setup But wait it gets better For Linux and Mac users Azure Files also offers NFS shares so everyone is covered no matter what platform they're on Just a heads up when you're setting up NFS shares you'll need to use a premium tier storage account and make sure you've got a configured virtual network in place to control access It's easy to do and it ensures that everything runs smoothly and securely So whether you're running Windows Linux or Mac Azure Files makes file sharing simple cost effective and reliable It's all about boosting availability while keeping things hassle-free Last but definitely not the least let's talk about Azure table storage Now when I say rows and columns you might think "Oh it must be like a relational database right?" Well not quite Azure table storage is more like your trusty filing cabinet but with a twist Each row in Azure table storage has a unique key kind of like each drawer in your filing cabinet But here's where things get interesting It's not your typical relational database There are no relationships no stored procedures and none of that complex database structure we talked about earlier So what's the deal well we're dealing with semistructured data here which means that columns can vary from row to row Imagine your customer info table One row might have a name a phone number and an address while another row might have totally different columns No rigid structure And here's another cool thing Partitions Think of them like organizing your filing cabinet into categories Partitions help boost scalability and performance So when you're searching for your data you include that partition key kind of searching within a specific category in your cabinet This speeds things up and reduces the work your system has to do for reads and writes Pretty neat right now let's talk about the key system in Azure table storage It's not like your usual do key Think of it more like a combination lock You need two components the partition key and the row key The partition key is like your category label grouping similar files into one folder And the row key is your unique identifier within that folder like a file's distinct name Together these two the partition key and the row key create a unique combination that ensures no two rows in the same partition share the same key It's like giving each file its very own lock combination to keep things organized and easy to retrieve I know that's a lot to take in It's a bit like jumping into the deep end with all of this talk of keys and partitions but don't worry Instead of just staring at slides how about we log into the portal and create an Azure storage account together I'll walk you through how to store data using all these awesome tools step by step All right let's jump back into the Azure portal where we're going to do our Azure storage demo For this I'm going to click on the create a resource button and I'm going to look for storage account And this time the first one that we are going to be creating is this one a storage account And let's go ahead and hit create on the storage account As always I'm going to first choose the resource group So in the storage account we're going to first add it to a resource group as usual for this resource And then we're going to give the storage account a name Uh let's call it blob for uh DP 900 Let's keep this resource in the west Europe region and select Azure blob storage or Azure data lakeink storage gen 2 as the primary service For the performance we are going to stick to the standard performance since this is just going to be a demo But remember we can use the premium uh tier for those NFC fileshares that we spoke about For the redundancy we're going to choose the locally redundant storage Um this is because again since it's a demo account we do not want to uh create backups in other regions or other data centers Locally redundant will suffice So let's go to the next page Now the most important thing in this second page is going to be the hierarchical name space This is actually the feature that gives you an Azure data lakeink storage gen 2 account If you leave this unchecked it will give you a Azure blob storage account And if it is checked it will give you an Azure data lakeink storage gen 2 account So I'm going to leave this unchecked for now because I want to create a blob storage and click next I'm going to keep the public access from all network so that I can access the storage account I'm not going to create any private endpoints For the data protection I'm going to uncheck all of the recovery uh options This is good if you do want to have some recovery for any of these of your data but for for this demo we're going to leave this unchecked So let's go to the next page We're not going to make any changes to the encryption type keep all the default settings here And then we're just going to go to review and create the storage account So we can see all of the configurations we have set here and just hit create So all right here we go This one didn't take as long as our Azure SQL database So it's ready Let's go to the resource And here we are in our storage account And this is a brand new storage account which doesn't have any data yet So let's look at the different services we have here So if I zoom in here you can see I have containers um fileshares cues and tables So let's take a look at the containers first So let's create a new container here I'm going to name this products Actually let's change that and call that data and hit create This is our first container and we're going to put our products data inside this data container So now we're going to move to the storage browser which gives us the ability to make changes and add data delete data or update data into our container Let's go to the blob containers And here we can see our data container which was just created So let's move into the container And here let's add a directory So we're going to call this directory products But before we create this directory let's take a look at the virtual directory This is a virtual directory A virtual directory does not actually exist in Azure until you paste or upload blobs into it To paste a blob into a virtual directory copy the blob before creating the directory So let's take a look at how this works I'm going to hit okay And let's go back to our root uh container data And you can see we do not have that directory we just created That's because it was a virtual directory All right let's upload some data I'm going to browse for some files and I have some product data available So I'm going to select my product one file and in the advanced section I'm going to say upload to folder and create a new folder called products data and then hit upload at the bottom All right Now you can see we have a directory created and that's because we do have a blob inside this directory If this directory was empty we would not see this directory But because we have a blob we see the directory All right So let's go back to the containers under data storage and let's go into the uh container Uh let's go into our um actually let's check out the um the the state of our uh folder here And we have three dots But if you click on these three dots nothing happens because remember this is a virtual directory right so we cannot have any settings at the directory level because it's a virtual directory So that's a quick feature that I wanted to show you So that's a little bit about blob storage Now let's go back and take a look at how Azure data link storage gen 2 works For this we are going to scroll down to this data lake gen 2 upgrade setting available under settings Just wanted to highlight this data lake gen 2 upgrade And we have three steps here So let's go through them one by one The first step is to review the account changes before upgrading So uh this is going to check if you have any features enabled which need to be disabled or anything like that Um you just need to review your account changes and then also agree um to the the certain features um need to be supported So that's complete The next step is the validation So if there are any features that are not supported by ADLS Gen 2 these will be listed here in the validation stage This can take a little bit of time All right there you go Our validation has succeeded We did not have any discrepancies So we can move to the last step which is upgrade the account During this upgrade the storage account will be off offline and the upgrade can take several hours to complete depending on how much data you have in the storage account and we cannot revert back to a blob account after we have started this upgrade All right there we go We have successfully upgraded our account to data lake storage gen 2 So let's go back to our containers Let's take a look if our data container is still there It looks like it's still there We have our products data folder still here And if we go inside this folder we'll see uh our JSON files are also still here There you go We can see our JSON files All right So let's go back to our containers And if I click these three dots I'm actually supposed to see um some options But before we do that let's upload some more data in this uh folder Um so I'm going to up upload the products to uh file JSON file So let's do that from the storage browser Let's go to blob containers Go to data Go to the product data folder and upload the data here So I'm going to upload my product two file and just simply click on upload There you go I have my product two JSON file available here So now let's go to the data storage containers and verify that we have these two files in our um container And there you go I can see these two files here All right So let's go back into our containers and to the products data folder And now when I click on the three dots I should see some options here because I have the hierarchical name space enabled And now this is an Azure data lake storage gen 2 So I can see the properties of this directory Um I can see the permissions I can see the owner who's a super user I can see the name of the directory Um and I can also set ACL uh properties and and also generate SAS tokens So this is because this is not a virtual directory anymore This is a real directory that we have created inside our um inside our Azure data lake storage gen 2 All right Next let's move on to file shares So I have my file shares available over here under data storage That's what we're going to look at next And the first thing we're going to do is create a new file share So I'm just going to click on the plus new fileshare button Give a name for our file share So I'm going to call it u files And let's keep it selected to transaction optimized And go to the next step Here I'm going to disable backup because I don't need backup since this is just a demo And we will review and create our fileshare It's just simple to create a fileshare Right And now to connect this fileshare you have a connect button available there If I click the connect button I will have three different um um user interfaces that I can connect this file share to I can connect it to Windows to Linux or Mac OS Um so I can choose what kind of authentication I want to use if do I want to use um uh active directory or entra ID or storage account and then also I can select uh which driver I want to se link this up to in that Windows account and then also I can uh go down and click on show script Now show script where I will um see the script that I will need to run within that Windows machine to connect to this Azure fileshare Okay And with that you will be able to access uh the Azure fileshare from that Windows machine So we also have an option for Linux and for MacOSS So that was a little bit on fileshares Let's go to our final service All right So let's go to our tables And here we are going to create a new table and I'm going to call the table um products Oh and there was a space there So I just remove that space and click okay because we cannot add space to our table names Here we have our new table called uh products and then I'm going to go inside this table from my storage browser again select tables and here I have my table available products So now I'm going to go inside this table and I'm going to add an entity and this entity is going to have uh one as a partition key one as a row key and then I'm going to add a couple of properties The first one being name and I'm going to add the name widget I'm going to add another property and this one is going to be um type or sorry price and I'm going to select double for the string for the type of the uh data and put 2.99 and I'm going to insert this um entity So you can see my entity available here So if I go to my table and go back into my products you can see I have one record has been added Let's go add another entity And this time I'm going to give partition key to be one and row key two Name is going to be knickknack and price is going to be 1.99 And I'm going to add another property here which is discontinued And I'm going to have it as type boolean and give it true or false value In this case true And I'm going to insert this entity All right So we have two records One record um has four um attributes and another one has five And you can see the discontinued one is empty for the first record and it's blank and we only have a value for the second one So this is a nosql option that we have from within our storage accounts And with this we have come to the end of our demo Uh thank you and I'll see you back in the presentation Now that we've seen Azure storage in action let's dive into our next lesson in the session Azure Cosmos DB So you might be wondering what exactly is your is Azure Cosmos DB and why are organizations around the globe turning to it for their data storage needs well let's break it down In the world of database management traditional relational databases have been the go-to solution for what feels like forever Don't get me wrong they're great in a lot of scenarios but they can feel rigid and sometimes demand a lot when it comes to performance tuning And in today's world where data is constantly evolving that kind of rigidity can be tough to manage Azure Cosmos DB is part of the NoSQL database family Unlike relational databases NoSQL uses different models to store data like documents graphs key value stores and column family stores But what does that mean for you in a word flexibility and oh scalability too without sacrificing performance One of the coolest features of Azure Cosmos DB is its support for multiple APIs This means developers can interact with Cosmos DB using programming languages they're already familiar with no matter how the data is stored behind the scenes So whether you're working with SQL MongoDB Cassandra or others Cosmos DB has you covered This flexibility is a gamecher And let's talk about performance because that's the cornerstone of any effective database Azure Cosmos DB uses indexes and partitioning to deliver lightning fast read and write operations Plus it can scale seamlessly to handle massive amounts of data without needing constant migrations or infrastructure overhauls That means that you can focus on your applications without worrying about whether your database can keep up But Cosmos DB isn't just about speed and scalability It's also about global accessibility You get multi- region rights with just the click of a button That means you or your organization can add as your regions wherever they are needed This global distribution ensures that users across the world can access their data through local replicas creating a smooth low latency experience no matter where they are Azure Cosmos DB frees your organization from the constraints of traditional databases and opens up a world of possibilities for data storage management and accessibility Now before we dive into the practical side of things I want to loop back to those APIs I mentioned earlier Developers can use their favorite programming languages to work with Cosmos DB regardless of how the data is structured So let's take a closer look at some of those APIs that Azure Cosmos DB supports First up we have Azure Cosmos DB for NoSQL Think of it as your Swiss Army knife for your data It handles JSON document formats and speaks SQL making it incredibly versatile for all kinds of data tasks Whether you need a screwdriver or a can opener NoSQL's got you covered with its multi-functional capabilities Next there's Azure Cosmos DB for MongoDB This is like your chameleon friend who adapts to every environment With MongoDB data is stored in BSON a binary version of JSON and you can use MongoDB client libraries to interact with it in Cosmos DB It seamlessly blends into whatever database environment you need making data storage and management a breeze Then we have Azure Cosmos DB for Postgress SQL This one is like your librarian who keeps everything perfectly categorized and organized Postgress SQL is a relational database wizard giving you the structure and organization you need to store data neatly just like how a librarian knows exactly where every book belongs Now let's talk about Azure Cosmos DB for table Imagine it as your speedy career service for key value tables It delivers what you need right when you need it with incredible speed and scalability It's like having a delivery service that can bring you anything in your massive data warehouse faster than you can imagine Moving on we've got Azure Cosmos DB for Apache Cassandra This one's your carpenter It builds databases that are custom fit to your needs with each piece able to look different depending on what you need to store Just like a carpenter crafting each piece with care Apache Cassandra handles distributed largecale data with flexibility and precision And finally we step into the world of graphs with Azure Cosmos DB for Apache Gremlin Think of this as your detective who maps out all the relationships and connections in your data Whether it's people places or things Gremlin helps you navigate the web of interactions like a detective solving a complex case with a detailed map of clues Each of these APIs is a specialized tool designed to solve specific problems Whether it's managing documents organizing relationships or handling complex tables Cosmos DB has a right solution to meet your needs Now let's jump back into the portal one last time today and I'll show you how to work with these incredible tools within Cosmos DB All right so we're back again in the Azure portal And this time we're going to be creating an Azure Cosmos DB So let's create a resource and let's look for Azure Cosmos DB and it's exactly this first one that you see over here which says as your Cosmos DB just to highlight it and I'm going to hit the create Cosmos DB and we'll have a a couple of different options here different APIs that we have with Cosmos DB So we have the Cosmos DB for NoSQL This is the one we're going to be using for this demo We also have Azure Cosmos DB for MongoDB along with others like Cosmos DB for table Cosmos DB for Apache Gremlin Cosmos DB for Apache Cassandra and Cosmos DB for Postgress SQL For this demo we're going to stick to your Cosmos DB for NoSQL So let's create this The first thing we're going to do as usual is add our resource group and we're going to create an account name So we're going to say for this one it's going to be Cosmos DB for DP900 And we're going to um uh try to find a region here that works in Europe So we're going to stick to um UK West in this case So let me just find UK West Um there you go UK West And I am going to keep this as provisioned throughput I'm not going to apply the free tier discount but you can if you want I'm not going to apply it Um and I'm going to go to the next which is global distribution I'm going to keep both of these as disabled but these are really cool features you have for Cosmos DB which is geo redundancy and multi- region right super powerful features for Cosmos DB for global distribution So if you have an application which you're using in multiple regions having multi-reion rights is really really powerful because it gives you very low latency features there In the networking tab we're not going to make any changes here and we're just going to click review and create We're not going to make any changes to the rest of the tabs So validation was successful and we can create our Cosmos DB account All right So we have our resource Let's go to our resource And now that we have created Cosmos DB account the first thing we have to do is create a container In order to do that we are going to go to the data explorer which is an option we have right here So I'm going to just highlight it Oops I'm just going to highlight it The data explorer right here And um in here we have a little bit of a video that usually shows up so you can use it to understand what Cosmos DB is I'm just going to close this And what we're going to do is we're going to click on this launch quick start to create some uh a container with some sample data in it So you can see this is already prepopulated sample DB for my database ID I have sample container for my container ID and uh we also have the partition key category ID And I'm not going to make any changes to the default settings here I'm just going to go click on okay Just going to give it a minute for it to actually create this container with some data in it There you go You can see our sample database and sample container getting created in the background And there you go Our sample data um container has been created Going to close this uh view sample data And let's explore this manually I have my sample uh DB which is already opened up So I close it and open it You can see this is my sample DB Within my sample DB I have a sample container and within the container I have my items and you can see this is one of the items that you have here I have ID category ID category names SKU name description price and tags Now these columns that you can see right at the end uh with the underscore these are generated by Cosmos DB itself So this is not uh columns that you or attributes that you are creating and adding to um the the records So here let's create a new item here uh just to show you how to create a new item So I have the new item button there I'm just going to click it Just want to show you the new item button And here we are given an option for populating two attributes ID and category ID Why is that because when we created the sample DB we actually gave it uh category ID as a partition key All right I've populated just 1 2 3 4 5 6 7 here for ID And I'm just going to populate u another uh random key here But this is mandatory because it is a partition key um value in your database So let's create some additional attributes here Um so let's give it uh for example a name and I'm going to call it um um road helmet 45 And I do need to add a comma there And then um let's add another one which is SKU And I'm going to put a random SKU number there that I'm just going to create End with a comma And then I'm also going to add a description And I'm going to say this product is called road helmet 45 And then end with uh the last coats So you see the backslashes uh that I've added just before the quotes the double quotes That's so that I can recognize those quotes and it doesn't end my string text there Okay and that is why I've added that backslash Okay The last attribute I'm going to put is the price This doesn't need any quotes because this is actually text So I'm just going to put the text right the numerical value right in here And that's it I can add my new record I'm just going to click on uh save and that's going to add my new record And you can see I've added this columns were added automatically to my new record without me having to do anything Right we haven't added this These uh fields are going to be super helpful for PowerBI to navigate and process the records Now that we've added a record let's go and see how to query this in the data explorer So let's go to u one of this options in the data explorer uh right on top to query the data So I'm going to go navigate back to the data explorer and go into my items And one of these uh options on the top um are going to be to query uh to create a new query not open a query There you go So new SQL query that's the option the new SQL query option Just want to highlight it here because it is difficult to find And then I have a query already generated for me So I can already execute this query I'm just going to put the from C on the next line and I'm going to add a wear clause uh so that I can find the record that we just generated So I'm going to say where C name um includes um the helmet word and execute the query So if I see the results of this query I can scroll down and you can see accessories helmets accessories helmets and this is the one that we created just now the road helmet 45 So that's the one we just added So that's how you can see it's super easy to query our Cosmos DB data in this query editor And with that we're done with this demo See you back in the presentation Wow that was really something Cosmos DB always gets me fired up It's so simple yet so powerful Now before we wrap up let's take a quick moment to recap what we have covered until now We laid the foundation by exploring the fundamentals of data We talked about the different data types how data is managed and the roles that data professionals play in the modern data landscape We also took a deep dive into relational data in Azure and ended with a look at non- relational data including services like Azure blob storage and Azure Cosmos DB and how to work with them