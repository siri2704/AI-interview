being good at data structures and algorithms is a computer science equivalent of having i mean everyone assumes you're a genius you get high paying offers from prestigious companies and you even have a high social market value on internet forums but the journey from lead cell to algo chat is not an easy one it's filled with frustration and imposter syndrome i know i've been there but now i have offers from feng which basically qualifies me to cure cancer so to give back and help even the most confusing programmers consider this video my cure since this industry is cancer [Music] the most important part of learning something new is that you actually want to learn it this is why i failed out of spanish one so before we start ask yourself why do you want to learn this don't do it to get a job at google do it because learning this changes the way you think data structures and algorithms is about solving problems efficiently a bad programmer solves the problem inefficiently and a really bad programmer doesn't even know why their solution is inefficient so let's start with that how do we rank an algorithm's efficiency [Music] say we wrote a function that goes through every number in a list and adds it to a total sum variable if you consider adding to be one operation then running this function on a list with 10 numbers costs 10 operations running it on a list with 20 numbers costs 20 operations say we wrote another function that just returns the first number in a list then no matter how large the list is this function will never cost more than one operation clearly these two algorithms have a different time complexity or relationship between growth of input size and growth of operations we communicate these time complexities using big o notation referring to input size as n common complexities are constant linear quadratic logarithmic n log n exponential and factorial our first algorithm runs in o of n meaning its operations grow in a linear relationship with the input size which in this case is the amount of numbers in the list our second algorithm is not dependent on the input size at all so it runs in constant time let's take a look at how many operations a program will have to execute in a function with an input size of 5 versus 50. it might not matter when the input is small but this gap gets very dramatic as the input size increases if n were 10 000 a function that runs in log of n would only take 14 operations while the function that runs in n factorial would set your computer on fire for big o notation we drop constants so o of 10 times n and o of n over 10 are both equivalent to o of n because the graph is still linear and bagel notation is also used for space complexity which works the same way for how much space an algorithm uses as n grows unless you sit in your room doing algorithms all day there's a good chance you forgot what logarithms are logarithms are the inverse to exponential functions let's say i wanted to find a word in a dictionary one method is to check every word one by one this is o n but nobody does that so how are humans able to find any word in a dictionary with a hundred thousand words in seconds we do something more along method two cut the search window in half each time by checking the middle element if we passed our word search on the left half otherwise search on the right half and then we repeat this until we find our word if our input n doubled in size that would only grow our operations by one in computer science this is the binary search but for the binary search to work the collection has to be sorted which opens up a huge field of computer science sorting algorithms a very basic sort is a selection sort you have one pointer at the start of the list and another pointer that is linear scan to find the next minimum element then you swap those elements and increment the pointer since you're doing a linear scan for every element in the collection this runs in all of n squared a more efficient algorithm is the merge sort a merged source splits a collection in half into sub-collections until those sub-collections can be sorted in constant time and then it works its way back up by merging sorted subcollections until the entire collection is sorted since it's splitting in half there will be log n splits and thereby log n merges it is o then to merge two sorted collections into one sorted collection since we're doing that log n times the time complexity is of n times log n no algorithm can sort an arbitrary collection in a better time complexity than that so we consider n log n to be the cost of sorting perhaps the most fundamental data structure is an array an array is an indexable contiguous chunk of memory arrays are fixed size you can't insert a ninth element into an array meant for eight elements a list data structure with a flexible size is a linked list the idea is to package your data into nodes that hold one your data and two a point that points to the next node traversing a linked list reminds me of those youtube direction chains like you see a comment that says read my profile picture and their profile picture says click on my profile which brings you to their header which says read my bio so you go to their bio and it says click the link below and you click it and then it installs a virus onto your browser if a note's next pointer points to null it's the end of the list you can add a new node to the list in constant time by just setting that pointer to the new node by doing additional tom [ __ ] with pointers we can also insert and delete nodes in constant time sometimes nodes also contain a pointer to the previous node in a variation called the doubly linked list but a major downside of linked list is that you can't access elements in constant time via index like an array in practice both programming languages have dynamically sized arrays or lists which work by creating arrays with a fixed size if the array is full capacity and you try to add a new element the programming language automatically creates a new array with double the capacity copies the current elements over and sets your pointer to that new array since this happens so infrequently we generally consider appending to a list to be a constant time operation link lists aren't the only data structure that include nodes referring to other nodes what if instead of pointing to a next node nodes pointed to a left and a right child node this is called a binary tree these child notes can also be called subtrees since trees are recursive in nature a node with no children is called a leaf node while a node with seven children is my father-in-law a common type of tree is the binary search tree which follows these rules basically a node's left child must have a smaller value while a node's right child must have a greater or equal value let's say you're looking for element x you start at the root and if the number is smaller than the root you go to the left tree if it's larger you go to the right and you keep repeating this until you arrive at your number in the worst case you have to traverse the height of the tree so the time complexity to search insert and delete elements is of h where h is the height this is efficient because on average the height will be log n but what if you inserted every element of a sorted array into an empty binary search tree well that's a linked list meaning the height would be n but we can guarantee log n time complexities using a self-balancing binary search tree such as red black trees which maintain additional properties to guarantee that the height of the tree is o of log n binary search trees are kind of considered to be a workhorse distribution that can solve most problems with decent efficiency but i found situations where binary search trees really shine are when you're asked a question about binary search trees another type of tree is a heap the primary difference between this and the binary search tree is actually use this one it's also sometimes called a priority queue because at the root of it is always the highest priority element and min heap will have the min element and the max heap will have the max element at the root but don't get me wrong the rest of the heap is unsorted it is an absolute wasteland down there searching in the rest of the heap may as well be an unsorted array we only care about what's at the top just like capitalism to insert a new element into a heap first we find the next available spot to add a leaf node then we compare it with his parents and if it's a higher priority it swaps and bubbles his way up we're doing at most log n comparisons you can build a heap from a random collection by inserting n times which cost o of n log n but there's also a way to build a heap in o of end time i'd suggest reading this wikipedia page because it's super interesting stuff and lastly since the heap is nearly complete although we can visualize it as a tree we could actually use these properties to represent it with an array one way to traverse a tree is a depth first search which is like going deep fully exploring one path and then moving on to the next one way to visit this tree in a depth first order could be to start at 10 go to 13 go to 4 then 6 then 12 then 8 and then 1 whereas another option is a breadth first search where we view it more level by level a way to print this same tree in a bfs manner could be 10 13 12 4 6 8 1. when it comes to implementation depth first search can use a stack a stack is a list-based data structure where the only two operations are add to the end and pop from the end this makes a stack lifo last in first out dfs are often done using recursion which indirectly uses a stack the recursive stack so if your recursion exceeds a certain amount of depth then you get a stack overflow let's look at a way to print this tree in a dfs matter using a stack so first initialize the stack and add the root then while there are still elements in the stack pop from the stack print that element and then add its children to the stack on the contrary a breadth first search uses a queue a queue is fifo first in first out so instead of popping from the end of the list you pop from the beginning doing the same algorithm as before but using a q instead of a stack the tree will now print in a bfs order all trees are graphs but not all graphs are trees a graph is a collection of vertices which are like points and edges which are like connections from one vertex to another vertex a graph can be directed meaning edges can only go one way so this edge means you can only go from a to b or undirected meaning you can also go from b to a two common ways to model edges are adjacency lists and adjacency matrices graphs are my favorite part of destruction algorithms so i'd like to show how cool they are with three example scenarios on where you can use them so it recently dawned on me that i know kanye west and by that i mean i know joma tech but joma tech knows jarvis johnson who knows mkbhd who knows elon musk who knows kanye west which is five degrees of separation between me and kanye west i wanted to find something shorter well turns out i know bretman rock who knows rihanna who knows kanye three degrees of separation which also means that anyone who knows me is at most four degrees of separation away from kanye if we wanted to make a program that computes the smallest degrees of separation between two people a graph would be the perfect choice you model people as vertices and you model relationships as edges the shortest path between the source node me and the target node kanye can be found with a breath first search since we're moving out level by level we can guarantee that the first time reaching the kanye node will also be the shortest path the problem with implementing this algorithm the way i just described is that nobody has a concrete list of all the people they know nor would that be publicly accessible by me but a possible variation could be in a social network like facebook where we can use the friend list as edges we would then be able to run this algorithm and find the smallest degrees of separation between any two users in that social network imagine a map system like google maps that wants to find the shortest path between two locations this is different than the last example because although they both deal with shortest paths the degrees of separations did not have weighted edges where a map system does for example if we're computing the shortest path between two vertices and there's a direct edge between them weighing 20 versus a path of two edges weighing eight each a breath first search would give us the shortest path in terms of the vertices away but not the smallest weight one algorithm that computes the shortest path in a graph with positive weighted edges is dijkstra's algorithm using the heap data structure in the original version of this video i explained dijkstra's and my video shot up 6 minutes so instead just research it for yourself if you're interested but graphs aren't just good for path finding imagine a course schedule in school with classes and prerequisites for each class and say you wanted to find the order in which you can take all your classes while still fulfilling your prerequisites well model the classes as vertices and prerequisites as edges count how many incoming edge each vertex has add vertices with zero incoming edges to a queue then pop and print the element from the queue and decrement the incoming edges of all its children if a child now has zero incoming edges add it to the cube and repeat while there are still elements in the queue this algorithm is known as a topological sort [Music] hash maps are sort of the holy grail of data structures with basically constant time retrieval of data the saying goes that if you don't know what you're doing just try throwing hashtags at the question as an example one time i was in a coding interview and froze so i just told the interviewer hmm i think the solution to use a hash map unfortunately the question was what are your biggest weaknesses so the better answer was hang up the phone to show that i have none a hashmap is a data structure built on top of an array optimized to store key value pairs what makes it so powerful is you can retrieve delete and store data in on average constant time here we have a map where keys are strings representing people's names and values are their corresponding ages we can directly access trend black's age like this it's almost as if strings can be used as indices but how is that even possible because of this little thing called a hash function a hash function will take a key and return a hash code if we take that number modulus the length of its underlying array we can use that as an index to store its value but the hash function has to compute the same hash code if given the same key so hash of trend black should always return the same hash code or else we lose the index but what if hash of trend black and hash of ziz both end with the same index well this is called collision one way to deal with this is to store our value in a linked list so when we go to stores is and see that index three already has a value we have that value point to the new value this is why a hash map is not strictly of one because if you write some god awful hash function then it won't be balanced and we will have to do a lot of linkless traversing good hash functions evenly spread out hash codes in practice modern languages use very good hash functions so you don't have to write your own an example of a hashmap is the dictionary in python or the object in javascript and remember constant lookup of data is very overpowered so try to use it when you can [Music] and that's pretty much all you need to know for data structures and algorithms a six month college course taught in 13 minutes now of course you didn't learn anything in great detail but trust me it is a lot easier to learn something in great detail if you already learned the big picture first this is why college sucks at teaching it dives very deep into one topic and then moves on to the next it's like a depth first search i believe the better way of learning is to get a general understanding and then build on it like a breath first search so if you want to keep going what are my recommendations to build on your knowledge well the first one is jomo class you know those really nice animations in this video like this one yeah i got them from joma class if you don't care about getting extremely theoretical and just want simple to the point everything you need to know and nothing you don't type lessons then jomo class makes this difficult topic very accessible but i will admit that i roasted the [ __ ] out of jomah a while back because he was pushing some actual snake oil well unlike some people he took my criticism very well and to vindicate his name he made something that is actually valuable and i will defend that he put much more effort into these explanations dropped the price from a thousand dollars to eight dollars a month and most importantly he dropped the dead weight if you're a complete beginner it also comes with a course on how to code in python and of course on sql if you're a bit more advanced but i gotta say the best part is the community aspect each lesson has a whole community behind it where people ask questions discuss topics and just learn together well i like the course so much that i actually called joma and i was like yo what the [ __ ] dude this is actually good i'm gonna start recommending this to people and he was so excited to hear that i liked it that he was actually willing to offer my audience a discount so if you go to trend.jomoclass.com then you'll get 15 off but even though paying for stuff increases the chance that you'll actually complete it you don't have to spend money not everyone has eight dollars a month to drop and i want to assure you that you can still learn everything for free so it might take a bit more self-discipline but you can do it so don't listen to anyone who says you have to pay so if you do want to go down the free route i'd recommend these college lectures they're literally taught by mit professors and posted for free online they're pretty theoretical but very comprehensive and it's perfect if you like that old-school chalkboard setting so yeah good luck guys and go get that guck 3000