all the presenters everyone that attends this conference is so friendly so willing to teach um so willing to learn from you too so it's not just watching these videos it's kind of getting that Insight afterwards and around at dinner at drinks um like all the other aspects of the conference besides just like that technical [Music] information all right thank you so much everyone for coming I understand that I'm all that stands between you and dinner so I really appreciate you making it um but I think we have some really interesting stuff to talk about today um I'd like to make this interactive so if you have any questions please feel free to you know raise your hand and ask during the presentation um but I think we should also have some time for questions towards the end um so hi everyone I'm Ben Brock I'm a research scientist at Intel Labs um inside our parallel Computing laboratory where I work in our AI software group um we work on compilers and libraries for AI and other large- scale problems and I specifically work on distributed C++ data structures and algorithms um specifically for kind of HPC uh type uh scenarios where we want to offer a high level of abstraction but still get uh reasonable performance running across multiple nodes or multiple devices um so what I'm going to talk about today is this um thing we've been working on called distributed ranges where we try to extend the ranges library to support um data that is distributed over more than one memory local that could be multiple nodes in a cluster or multiple devices um inside a single node and then to basically Implement we're not all the way there yet but implement the ranges Library so the algorithms the views um so that they all work together um this is my obligatory notices and disclaimers I've been told if you stare at this QR code long enough you can see Albert Einstein um but here are my human readable disclaimers right the views in in this talk are mine they don't necessarily represent my employer um this is a speculative academic style talk and I'm not describing anything about future Intel products um I probably don't know anything about future Intel products um and I work in the the research Labs so I'm talking about experimental prototypes in early research um so parallel programming is hard um right this problem of taking your code and running it acoss across a cluster of computers um requires a lot of work on the user's part and parallel Computing is getting more and more necessary as our systems become more and more hierarchical if you look at a modern kind of server box right you you'll typically see a layout like this system diagram on the right so you have uh one or more CPUs you have some gpus these are connected with a uh kind of high bandwidth interconnect um but to get good performance users are still going to have to manually split up data amongst multiple gpus um and this creates a lot of burden and so we think it's important to have high high LEL mechanisms like distributed data structures that will help um ease some of this burden in distributing your data and then Computing over this distributed data so if we look at um this system right as I was saying you've got four gpus so even if you're just on one node now you need to think about these four different devices and what they're doing um and as our devices get bigger and bigger the devices themselves are becoming hierarchical right so if you look at uh recent gpus they have things like chiplet architectures um some CPUs do as well which gives them Numa characteristics or even essentially their own memories right and it's most efficient if one of these tiles of the GPU is accessing its own memory as opposed to the memory of its uh neighboring tile and so if you want to get the best performance you now need to think about where does my data live and how do I partition my data across these different memory domains or memory spaces okay so we want to solve this problem of basically taking high level standard C++ having distributed data data structures that will automatically distribute the data and then achieve high performance both on these kind of multi-gpu and Numa scenarios and also on multi-node execution so basic we take something that looks like this right written in standard C++ using ranges and then execute it on uh a super computer um this is a outline for the talk um I'll talk quickly about some background both just some background on the ranges Library so we're all on the same page and what's in the the um the library today um and then I'll talk about um parallelism and distributed data structures how we can build distributed data structures in C++ um and then we'll go into the actual Concepts that we're relying on how we Implement on top of these Concepts and then how this generalizes to some more complex data structures um and then we'll have some some wrap-ups some lesson learns and some uh you know uh wish lists for maybe a future ranges paper um so let's let's talk about some background so in uh standard C++ I would argue you essentially have three components three Tools in your toolbox that help you implement a program and then in standard C++ and then run it on different architectures um uh you know from different vendors so you have data structures right which hold and organize your data you have views which are lightweight objects that typically represent kind of Transformations applied over your data although they can just be like represent representations of freestanding functions um and then you have algorithms that operate on and actually modify the data right um let's talk through this example here so we're using two views a zip View and a transform View and then an algorithm or reduce to perform a DOT product so what we're we're doing here right is we take our two inputs our two vectors X and Y we zip them together now zip is a view so it doesn't actually perform any computation all it does is create a semantic representation of these two ranges zip together so what you're going to see is effectively a range that contains um a pair that has references to X and Y right and so when you access those you'll be accessing the corresponding elements of the vectors X and Y um we then can perform a transform using an element wise multiply so then we take each corresponding element and multiply them together right this is performing that element wise multiplication of X and Y and then reduce all of them together in an algorithm right so together this uh performs a do product operation just using standard C++ um now the thing that holds all this together is um iteration and more recently the ranges Library which formalizes some of these Concepts um so the ranges Library formalizes iteration and also ranges which are objects you know that are collections of data that you can iterate over um the nice thing about uh the the ranges library and the algorithms is that they are extensible so execution policies can allow parallel execution um here we're using the par un seek execution policy which will which basically tells our library that it's free to parallelize uh this algorithm and computed in an unsequenced fashion which basically means a vectorized fashion um and also the standard allows implementation def find execution policies on top of this right so what uh multiple Hardware vendors have done is go and Implement uh implementation defined execution policies for their devices so for example from Intel we have an implementation of standard Library algorithms called 1 DPL and we have a device policy which represents execution on uh some sickle device right which could be a GPU or could be a CPU could even be an FP GA and then um we can we can basically take the standard C++ code and run it on a GPU now there is one thing that I want to uh kind of posit here is a caveat which is this question of where is the data here we're kind of glossing over it although we'll try to get a little bit more formal about this question of where the data is um later on but it's important to to keep in mind right if you're going to be executing on a device then you're going to want those uh ranges that hold your data to be in devices that correspond to where you're executing right so if you're running on the GPU you want your data to be on the GPU okay so C++ 20 added the ranges Library um and arrange is a collection of values and they're essentially they they kind of bind together these three different things that you can do you can iterate you can use an algorithm and then you can have views over your ranges um there's a hierarchy of different kinds of ranges right um which correspond to the different styles of iteration that you have and also a few other categories like size ranges um the and these categories are essentially refinements of the uh lower types of ranges if you will that add additional features to them so for example a random access range is a bidirectional range that also allows you to access any element in constant time um a contiguous range is is a random access range that corresponds to a continuous block of memory um and for parallel Computing we're mostly going to be interested in things that are like random access ranges because it's useful to be able to split up your data into chunks um and access any Chunk in constant time right so that you're not trying to access like a singly linked list in in parallel okay um so let's talk a little bit about parallel Computing and get on the same page about how we're going to communicate and move data around uh in a parallel program so um typically we're executing on a collection of nodes or devices um these devices are going to be connected by some kind of a network and then uh they're going to you know perform computation and communicate in order to uh process data um I kind of did I did a cppc talk back in 2021 on uh distributed data structures and C++ um this section is a little bit of a recap of that talk um but I discuss kind of in more detail there but very briefly there are two kinds of communication mechanisms uh that you can use for um executing over multiple nodes there is message passing in which processes send and receive data simultaneously so we have node number zero here which is sending some collection of data and node number one which is receiving the data uh the key to message passing is that the processes typically have to be aware of uh the other process that's sending the data right so both process zero and process one say yes I would like to send a message I would like to send it to process one and then process one says I would like to receive a message from process zero um so the key is that they're both participating here um in contrast RDMA a remote direct memory access allows processes to directly read and write into other processes memory so for example if I have uh two nodes in my cluster one one is node number one and node number one has exposed part of its memory as a shared segment node zero can say hey I would like to write the value 12 into this offset in node one's shared segment um this is what this might look like in your code um if you had a kind of remote pointer API right and the key here is that process one doesn't participate in these remote rights um I would argue that building uh these kind of RDMA based distributed data structures is um kind of fundamentally more fitting to the way that C+ plus thinks about memory right because typically we have memory and then we have ways to write over that memory as opposed to having these additional constraints that people are sending messages back and forth um so um we're going to be using RDMA to build distributed data structure or RDMA like Primitives where we have these special pointers that point to data that might live on another device um when thinking about how we might Implement something like this uh we're typically going to have a class or a struct that um is a remote pointer and essentially what this is going to contain is some information about which process or which device this data lives on which we're going to call a rank and then an offset within that rank's shared segment um we can then go and Implement um all of the normal memory communication operations that we would expect um like M Copy for example and then whenever we need to perform these operations we're going to go and call into a communication Library which is going to perform an RDMA primitive right so if we're using npi's one-sided protocol which supports RDM based operations we might call NP NPI get which is going to get the uh a collection of bytes from the uh Target buffer on this remote node and copy it into a destination buffer on the local node now we can also uh make these remote pointers um iterators actually so we can add a dreference operator and then uh the the limitation here is that we can't return a raw language reference the reason for this is that this memory lives in another memory space it doesn't actually live in like the the C++ uh you know memory land of normal pointers so we can't get a a raw language reference to to it what we can get is a proxy reference a kind of smart reference that uh represents a reference to that remote memory right and then um inside of that remote reference we can go and Implement uh an assignment operator which will write to that reference so that would uh if we if we have a remote pointer from somewhere and we want to write a value to it we would end up copying some data to this remote uh location and then if we want to read from it we can Implement a um implicit conversion operator which will read the data um and return it uh uh to the uh you know to the local node um this does um limit us to trivial uh trivially copyable types um because we're depending on mem copy right you can't have like a string that has um like dynamically allocated memory associated with it with this simple model um there are some tricks you can do to actually represent more complex objects um and I talked about those in my previous cpon talk but we're not really considering those today okay so just to recap right we're we're ex we're executing this domain where we have uh data which lives on different nodes and different kinds of devices and and then we have remote pointers that we can use to represent that remote data these can support MIM copy copy atomics all the things we would expect from a normal pointer or iterator in C++ and they can support dereferencing uh with proxy types um and like I said on the previous slide they're limited to trivially copy types now I want to stop for a moment and reflect on exactly what these remote pointer things are and also to point out that they do not fulfill Legacy Random Access iterator or as it's called in the standard uh CPP 17 random access iterator in fact they don't even uh fulfill Legacy forward iterator because Legacy forward iterator requires that when you dereference the object you get a raw language reference right so that's a sad face however the ranges library is designed differently right so ranges deals with things where you might not actually have underlying data so if we think about um just one of the views that we might have uh which is a zip view like we saw in the previous uh slide um here what we're doing is we're taking two vectors and then we're zipping them together right if we uh return a single element in this view um what is the thing that we're getting back well it's a pair of two uh two elements from those previous vectors but it's actually a stood pair of references not a a reference to a stood pair right so we've already broken this requirement that um we have uh you know Chi Ampersand as our reference type um so the the iterator the new iterator Concepts that we have right are are less strict and specifically what they say is that um uh a forward iterator just needs to be an input iterator right which requires that if you if you do star iter it returns something which is valid and non-void um so our we will fulfill uh the the iterator concepts here for these remote pointers and proxy references are fine okay um so just to recap one more time so these remote pointers can reference memory delived on another node they fulfill the random access iterator Concept in the ranges library and we can use them to build a random access range right so we could do something like create a sub range where we have uh two remote pointers that we pass in and this will this will work just fine um I do want to point out that naively using them like this might not be the smartest idea right because if this remote pointer is referencing memory on a GPU each time you dreference it you're going to be performing a copy operation from your GPU to your CPU and vice versa which is going to be slow so this is a nice property but it doesn't do everything that we want yet um so typically distributed data structures are going to split up data across multiple segments right so whether you have a distributed array a hash table or a matrix right uh these many different kinds of data structures they all have segments right and each of these segments is typically stored in a different memory region right um so if you have uh a distributed array it might be split up across uh a few different nodes um and what we want is a unified concept for accessing accessing all of these different kinds of distributed data structures right um typically what is done is somebody implements a distributed data structure and then they Implement some algorithms for the distributed data structure that are specific to the data structure um what we'd like to do is have AEV a level of uh genericness like the standard Library where we can Implement an algorithm that will work for um multiple different data structures okay so our data structure is partitioned across multiple segments each of these segments is remotely accessible and it's located on a single rink um we can access each of these uh bits of data if we want using these remote pointers um and uh this essentially gives us kind of these are the base primitiv that we're working on when we're implementing distributed data structures um data structures just to give you some idea of how uh distributed data structure would be implemented um typically we'll have a vector of segments of some kind right where the segments split up your data across many different uh devices or many different nodes and then you uh when you want to access a bit of data for example a bracket operator where you want to access a particular um location within your distributed Vector um you go to the appropriate location within this set of segments and then operate on the data um you can build um like a normal Global set of iterators on top of this um but those are going to be slow and are not necessarily what we want yeah okay so uh like I said yeah with iteration our distributed data structures are now ranges um but this Global iteration is slow and uh not particularly useful and what we want is a a generic concept for uh distributed data structures and distributed ranges okay so now we're going to talk about that uh so I want to start from the bottom up so if we have uh say a distributed Vector it's going to consist of a bunch of segments and I want to talk about what this thing highlighted in red is one of these segments so uh what we came up with is that it's a remote range which is a range with an with an additional concept of locality right which we call rank uh using a customization point so um a remote range just has to be a forward range and then it has to have a rank so given that we have um a range we can a remote range we can say ah where is this remote range and um where do we want to operate on it so this this rank customization point returns U some implementation defined information about where the data is stored um for us it is a weekly increment value basically an integer that gives the implementation some information about where it's stored um I do think we could uh formalize this further um but we can now essentially ask given a remote range which of my execution agents do I want to execute on right and then we can uh if we have say that GPU with two different tiles we can go and launch our kernel on the right tile based on where the data is now a distributed range uh also only adds one thing it itself must be a range but it also has a segments customization point and what the and what the segments customization Point does is it returns a range of remote ranges um what this does is expose the distribution and also the locality of the distributed range um so essentially you see all of these different segments split up um over the different uh ranks that you have in your computation so given a a distributed range we basically have two things we have this global view of our data and then we have this segmented view of of our data um and the segmented view exposes the distribution um and the nice thing about this is that it allows us to use um uh components that we already have like the ranges library in implementing our distributed algorithms so we can basically split up this data and then uh execute on each of our segments using normal C++ algorithms okay so let's talk a little bit about um the implementation so what we've done so far has focused on essentially uh this set of Concepts along with views that are applied over these Concepts and then two different execution models a multi-gpu execution model built on top of CLE which we call SHP and then um a multi node multiprocess execution model built on top of NPI which can also use CLE to execute on gpus okay so if we have our distributed Vector like we saw on a previous slide and we want to make it a distributed range all we have to do is Implement a segments uh customization point so that we can then go and tell um where our different segments are and so this is pretty trivial for a problem like this right um we can just go return a view of the segments that exist within our distributed vector and then that is going to give us um all of our different segments um we can also Implement Global iteration um using a join view or some other method um basically by join together all of these different segments together right and then returning iterators to this join view um to implement algorithms right um instead of accepting range uh types into our algorithm we we now accept distributed ranges right um and then what we're going to do is access data using the segments customization point to see how our data is split up and then launch data on execution agents that correspond to where the data lives so for something like reduce um an implementation a multi-gpu implementation is fairly straightforward um essentially what we have to do is look at our segments using the segment CPO um and then assuming that our segment has data that we want to look at um find out where that data is go and get an execution policy that will execute where the data lives and then perform uh a like a reduction operation what this will do here is launch a kernel on the corresponding GPU that will perform a reduce right and then we can add up all of those results at the end um this does get more complicated when you have things like inclusive scan and sort um but essentially it's the same process of um operating over your segments um basically propagating your data and and uh communicating partial results and then copying back into the output um so it does end up being fairly straightforward to implement these algorithms um okay now distributed views are particularly interesting and I think are where this uh concept really uh proves useful so a view in the ranges library right takes a base view which we call V and then provides um a view of that of that object um that fulfills the range Concepts um not all views have a base view but for our purposes when we're where we're concerned about locality um we do care about uh specifically ranges where we like take in a range and or views where we take in a range and then transform it so we want to know essentially if if we have a transform view with some inputs where does this transform where does this transform view live and if it's distributed how is that data distributed um okay so if we have something like transform we can ask uh from the standard uh what what is the base right so transform is going to store a ref view inside of it and then we can then go and ask the ref view what is your base ultimately this will give us a reference to the stood Vector um now so if we have let's start with with remote views so if we have uh a view whose base is a remote range then we'd like that view to also be a remote range what we need to do to make this happen is to implement a rank CPO for it so basically if we have a view whose base is a remote Vector we would like for um the view to have a rank itself so that it's also a remote range um so we have essentially two options um the first option would be to implement all these views from scratch and add a rank or segments method which is not optimal or what if we could essentially implement this rank customization point for different Library implementations um it turns out that we can do this for many of the views um so if we if we have a view whose base view is a remote range then we know that that that view has to be in the same rank as the the the base view so right if we have a ref view which is a ref view of remote Vector it's going to be on the same rank as the original original remote Vector um so what we can what we can do is go and implement this rank customization point for any ref view where the base is a remote range right and then simply call rank on the base to go and get uh the the rank where it's located um for distributed views things get a little bit more complicated um so we can go and Implement a segments customization point for these views but it does get a little bit complicated for some of them and there are some limitations for example for transform um we unfortunately don't have a way to access the element wise function that uh you're performing over the vector with transform and so um given a transform view what we would so what we what we could do let me explain this code on the right he's given um a series of segments right from from the base we could then apply a transform a transform view to each segment right that would be equivalent to uh like a transform view applied to the whole the whole series of data um unfortunately we need this fun object which we don't have any way to get access to so we do end up implementing our own uh distributed transform view um yeah this fund is an exposition only private member um it would be nice if we had some way to access it um but what we do as a workaround is Implement our own transform view where we essentially do this same operation right we have a series of segments and then we apply a transform to each segment using this uh element wise function and since it's it's our implementation we have access to it um this is still a a fairly modular implementation um and is doesn't introduce too much code complexity um there are some other views where we can just use uh like the the standard View and then have um uh segments customization Point implementation for example things like take drop uh slice Etc um I I'll comment here one thing that I really wish we didn't have to do this with is zip uh ZIP is probably the most the single most complicated part of our whole code base um if you if we had a way of getting a hold of of all the base views for a zip view we would be able to implement uh a customization point in the same way unfortunately there's no way to do that and so we do end up implementing uh a zip view in kind of a similar way um which ends up being a little complicated okay so um in the the CLE code base right in SHP um we can basically execute this standard uh C++ program that we looked at at the beginning um it will be automatically distributed across multiple gpus um we essentially just have to use this new namespace right where these algorithms and these concepts are implemented um and then uh what this is going to do inside of the reduce algorithm is uh uh issue a bunch of calls to this one DPL Library which is going to execute um kernels on all of your different gpus to perform this do product um and MHP there's the same situation um there is the additional complexity in MHP um which I don't talk about in in depth here that it is a multiprocess spmd execution model which is pretty standard in HPC environments um but does require that basically all of these things be Collective operations so in a SPD model uh you have multiple processes and each process is going to execute each line of code so when we create a data structure it's a collective call and all of the um all of the different nodes will communicate to move their data around um but other than this uh caveat it is uh pretty much like standard C++ okay so I I won't dwell too long on this um but we've run a collection of uh standard C++ benchmarks by no means do we have full coverage um but we do think this is this is a really interesting example of how you can build on top of ranges um and execute what is you know standard C++ across multiple gpus or multiple noes um so we see good scaling uh in this scenario so now um I would like to try to do a demo which is going to be very exciting I'm sure because you never know if it's going to work particularly when you do it live okay so what this what this thing is that I'm oh I need to make this much larger don't I all right can we see that so what this thing is that I'm writing into is a tool called replay um which essentially takes lines of code that you put into it it goes and compiles them runs them and then prints out any lines of input that you have uh from that particular line of code um this is necessary as opposed to something like cling because what what we're doing now is um running using Intel's 1 API compiler which is based on CLE using SHP um and we're going to be executing on a system that I think has eight gpus uh using distributed ranges so we can create a distributed Vector like this and then it is I'll I'll say again the reason that it's a little slow is because we have a compilation in this process right so for every line I I put in there's a call to cling Plus+ um but so we can create a distributed vector and then what this is going to do is it's going to create a vector um it's going to allocate buffers on each of our Ag gpus and then um we have this object that we can use to access the so this this uh distributed Vector we can use it uh in places we would use a normal range like uh print right so what print is doing is just printing it because it's a normal range right um but we can also access it using segments right so we could look and see how this range is actually distributed and say like rank something has segment something and then hopefully we should see how the data is distributed over our different gpus good now not particularly interesting because we have a we have a bunch of zeros so let's maybe put some data in our so we can we can do something like um an enumerate right which is going to take in enumerate view is going to take in our original data and then it's going to create a view of that data where we have um an Iota view zipped with the original data and then we can I don't know said V equal to I + 1 maybe hopefully I don't have any typos because the compilation is a while great so then we can print that out again and see what we have um but hopefully this gives you some idea basically you can you can at a high level create these distributed vectors use standard C++ algorithms over them um and you can also I didn't talk too much about how this actually works um but you can use multiple distributed ranges together and an algorithm um and we're able to do things like identify when the segments don't match up and then uh deal with that accordingly inside of the algorithms so for example we can do an inclusive scan um where we um basically have an inclusive scan operated over V and then we're writing the output into o which is a distributed Vector that is going to have a different layout right because it's much bigger it's 500 elements instead of 100 elements so each of its segments is going to be um you know much larger um okay the compilation time is getting a little longer as you can probably tell because as we add more and more kernels that the compiler is having to like generate code for um the compilation time gets a little longer um but we can just see what the output is here okay yeah so we can see that the output from the got written into the corresponding elements of O all right let's go back to the presentation great okay so I'll talk just very briefly um and I won't dwell on this because we're running a little bit low on time on uh dense and sparse uh matrices and how we can represent dense and sparse matrices uh using these distributed range Concepts um so first first off I'll talk about like how you might represent uh a sparse Matrix using a C++ data structure um what's weird about sparse matrices right is that they can be represented using many different formats and each format is going to have a different natural order of iteration right if you have a CSR data structure the natural way to iterate over it is by row because we have these row pointers which point to where uh the beginning and end of a row uh are in a values and a column indes array so this basically tell tells us that oh you know row number one uh is located from index one to three uh not including three in the values in column indices array and then we can see oh we have a two elements both uh titled both with the value one um at at column indices Zer and two right however there are lots of other different types of sports matrices that might have different natural iteration orders right for example uh coo you typically have just a set of tupal of values and row and column indices and then you iterate over those um just by individual Tuple not by row um so I've done a little bit of work uh with the graph BL community on developing C++ concepts for sparse Matrix iteration um and the basic way that we have um iteration by default is just unordered iteration through a series of Matrix tupal similar to the COO um there are more efficient ways to operate over some of the other data structures but by default this is the only thing that you can uh be guaranteed is going to be available for any particular sparse Matrix um you know if your Matrix supports um a particular type of iteration you might have some additional customization points right that might give you additional ways of iterating through it uh shout out to the graph Library proposal where they're exploring some of these uh techniques for graphs in particular um but what I want to show here is that if we if we have a representation of a sparse Matrix like this which is kind of a funky data structure we can represent and operate on it as a distributed range um so distributed dense and sparse Matrix uh data structures typically split up your data into a series of tiles so you partition your Matrix uh into tiles and then each tile is going to be stored in possibly a different memory space um and these tiles could be sparse if you have a sparse Matrix or they could be dense um each tile represents a submatrix right of your Matrix um and in a distributed Matrix what we can do is say that each tile satisfies remote range um so that we can then go and look right if we have a particular tile um tell me where that tile is and you can also do things like uh copying the tile to your local uh uh your local memory space so you can operate on it more efficiently um by creating a range that returns um all of these tiles right we can Implement uh a segments customization point for our Matrix right and then what this allows us to do is use one of these Matrix data structures uh with the normal range algorithms right so we can do something like uh have a distributed sparse Matrix and then apply algorithms in views like trans form and reduce on a matrix um which I think is pretty powerful um one thing that I kind of am glossing over here is that there is um this distinction between uh a tile view uh and a global view of the elements within a particular tile right typically when you operate on one of these tiles you use local indices um instead of global indices and so there is an index transformation required in the segmented View okay um so just to recap right standard C++ has all of these great things data structures and Views and algorithms that we can combine together to write code that is both high level and also able to be executed on um Hardware from different vendors right and and to get good performance um ranges are the glue that hold all of this together and so when we think about distributed data structures I think we need to be thinking about what are the concepts that hold together our distributed data structures and and what do we want out of those Concepts um in terms of lesson learned right um the ranges Library provides a lot of useful tools to build on top of um I think also the general design of using customization points is particularly powerful um and enabled us to do stuff here that would be very very difficult otherwise um it reduces a lot of um kind of repeated implementation that would otherwise be required um and also you know we can build on top of it we can take ranges Concepts and then refine them to add things like distributed and remote ranges um now while range has almost ranges has almost everything we uh we want right to build on top of there were a few things that uh would have been useful right so we ended up building uh some views from scratch in particular I don't know if there there may be compelling reasons that this is not the case but it would be useful if every view exposed uh its bases including zip right this would this would truly allow us to use um just customization points and then use like the raw Library provided set of views for all of our um all of our views and just build on top of them um it would also be nice right if there are things like a predicate or an element wise function that are an input to the view if those were also exposed um because I think those are very useful for libraries like this that are trying to build on top of ranges um limitations so um I did not talk at all about distributed iterator Concepts here today um we do have a distributed iterator concept but it is not as complete and nice as this distributed range concept um I do think that if view iterators exposed base iterators um which are currently just present in The View iterator implementations as like Exposition only private members that would do a lot towards allowing us to have like an expressive distributed iterator concept allowed us to support both the iterator interfaces for the algorithms and the ranges interfaces we do support some of the iterator interfaces uh in our code but it's not as complete as we would like it to be um another interesting uh question that we're trying to explore is uh how does how do how does this work if we have nested ranges like a distributed range of distributed ranges um given that our ranges can only have trivially copyable things inside them this is a bit of a challenge but I think it's important to examine um of course this is a research prototype although we're working to get this uh more and more complete and so we don't have full coverage um and also I think there are interesting opportunities for formalizing this concept of locality and like where our range lives beyond this kind of uh integer right that rank returns which gives us some implementation defined information about where this data lives um okay so just to wrap up um we've tried to extend ranges to support distributed data structures right ranges unites data structures algorithms and Views and I think it's important for distributed data structures to have something like that as well um we can build on top of vendor supplied uh algorithms to get competitive performance um our work is open source s so uh please do check it out and U we would love to talk to you about any interesting use cases or or problems that you have data structures or algorithms that are particularly useful for you um and with that I will open up for questions um but thanks every thanks very much [Applause] everyone use the official mic um so um so I have a first of all I think it's just kind of a really cool topic and excited to see it um so I have a few questions so how are you addressing uh ZIP operations and transformation and that span different ranks they don't have they don't have compatible segmentations and things how do you address that that's an excellent question so do I need to repeat the questions or can they hear on the mic um I'll just repeat so the question was about uh ZIP uh and basically how we handle cases where you have a zip View and the either the ranks are not compatible or the segment sizes are not compatible and so you have different cases so in the uh the GPU scenario it's actually fairly simple because we have uh like load store semantics for memory on other gpus and so we can simply slice up our our ZIP view so that we have a series of um uh basically um we have a series of ranges that are only on uh at most like two different ranks and then pick one of those ranks as the place where we want to execute um that's how we do it on the the GPU side and so there is there is going to be implicit communication that happens um when you um when you execute on one of those gpus right it's basically going to be using like normal load store to read out of another gpus memory which is fairly efficient like while you're executing that kernel um for multiprocess it's more complicated because um you you don't have like efficient load store semantics for another process's memory typically um so there are basically two things that we can do um you can either um basically execute on execute on one of those processes and then like read those values individually which is not particularly efficient or you can try to do something smart like copy them all over um in practice the segments tend to be aligned so we don't have this problem as much but it is it is one of the more interesting and complicated parts of this so okay so actually I do have a follow up on this thing so and I guess the I do come from more of a kind of know distributed computing that's more mpmd as opposed to kind of GPU space um but in that space you typically have to deal often with like redistribution data segmentations you end up having a lot of segments you know the random act is no longer linear it's quasi linear and so so have you thought about that space or that's kind of not not that that much kind of your thought that you so do you mean more in terms of like redistributing data based on right so if you do if you do a sort or if you do kind of any operation that kind of requires a lot of data Exchange what is kind of what are your thoughts in terms like who who's supposed to deal with that part yeah so that's an excellent question so when you do sort you might actually want to change your distribution right because the you might have a distribution that is somewhat like skewed typically you want to do something like a partition sort right you you look at your data and then you figure out like what am I going to pick as my my like partition points for all of my different segments and then you like send your data to a particular node based on that and then do a local sort and you're done right um what we what we currently do because the the sort API the user provides both the input and the output um we essentially like redistribute the data using possibly a different distribution and then then at the end we copy into the uh final output um and we don't we don't currently like have a mechanism for like changing the distribution of a distributed range but I think that is something that would be useful um and like maybe we should look into yeah thanks for the presentation I'm just curious uh thinking about the uh implementation of your distributed data structures can you implement all of that in just uh standard C++ and compile it with an off-the-shelf compiler or do you need to get special compiler sport support somewhere along the way yeah so what we depend on more I mean for the GPU stuff we depend on a compiler that can compile like GPU kernels um so we we we run using CLE which can be implemented using C++ standard C++ but typically like for if you want to run on gpus there's a compiler extension that compiles that um so for the the multi-gpu implementation we F we depend on CLE for the multi-node implementation we just depend on MPI and so you can compile like with GCC um standard compiler and just the NPI Library thanks okay well thanks [Music] [Applause] everyone